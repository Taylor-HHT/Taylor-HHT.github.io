<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>调研MSA中的方法（四）：对比学习</title>
      <link href="/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>这一章介绍四篇MSA中经典的结合对比学习方法的文章，末尾附上这四章的小总结~本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="4-对比学习"><a href="#4-对比学习" class="headerlink" title="4. 对比学习"></a>4. 对比学习</h4><h5 id="（1）ConFEDE-Contrastive-Feature-Decomposition-for-Multimodal-Sentiment-Analysis-ACL2023"><a href="#（1）ConFEDE-Contrastive-Feature-Decomposition-for-Multimodal-Sentiment-Analysis-ACL2023" class="headerlink" title="（1）ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis (ACL2023)"></a>（1）ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis (ACL2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;特征分解，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;差异性unique和一致性common 信息对于整体情感的含义非常有必要，尤其是对于sarcasm任务，</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;将每个模态分解为一个相似性特征（similarity feature）和一个相异性特征（dissimilarity feature），并以文本的相似性特征为锚，建立所有分解特征之间的对比关系。</p><p><img src="F:/files/myBlog/images/1ff561b2894ddb418ae717d2b428e1d9.png" alt="image-20240319221241287"></p><p>如上图所示，文章的贡献是在中间的部分，利用不同的投影器将每个模态的初始特征（T , V , A）分解为相似性特征（$T_s, V_s, A_s$）和相异性特征 （$T_d, V_d, A_d$）。（每个投影器都由 layer normalization，带有 Tanh 激活函数的linear layer 以及 dropout layer 构成），然后，通过对比特征分解学习不断地更新这六个分解特征，并将其融合到 ConFEDE 模型中，利用多任务学习目标损失函数进行训练。Loss损失计算对应图下面的三个任务，具体如下所示：$\mathcal{L}<em>{\text {all }}&#x3D;\mathcal{L}</em>{\text {pred }}+\beta_{\text {uni }} \mathcal{L}<em>{\text {uni }}+\beta</em>{\mathrm{cl}} \mathcal{L}<em>{\mathrm{cl}}$。其中，$\mathcal{L}</em>{\text {pred }}$ 为多模态预测损失， $\mathcal{L}<em>{\text {uni }}$ 为单模态预测损失，  $\mathcal{L}</em>{\text {cl}}$ 为对比损失。</p><p>① <strong>单模态预测损失：</strong>是将6 个分解的特征分别输入一个权重共享的 MLP 分类器，得到 6 个预测结果。<strong>相似性特征（$T_s, V_s, A_s$） 通过 MLP 映射来预测多模态标签  ，相异性特征（$T_d, V_d, A_d$）通过 MLP 映射来预测特定模态标签</strong> ，但是当特定模态标签没有的时候，例如MOSI与MOSEI数据集，相异性特征（$T_d, V_d, A_d$）也被用来预测多模态标签。这样做的原因是，<strong>让相似性特征（$T_s, V_s, A_s$）通过样本的整体多模态标签捕获不同模态之间共享的一致信息，让相异性特征（$T_d, V_d, A_d$）保留由单模态标签表示的特定模态信息。</strong></p><p>② <strong>对比损失：</strong>在一个简单的联合对比损失中进行对比：相似样本与相异样本进行对比（inter-sample）；同一样本相似特征与相异特征（intra-sample）进行对比。</p><p><img src="F:/files/myBlog/images/image-20241011155522619.png" alt="image-20241011155522619"></p><p>其中，(a,p), (a,k) 表示一对分解后的特征向量。$(a, p) \in \mathcal{P}^{i},(a, k) \in \mathcal{P}^{i} \text { or } \mathcal{N}^{i}$</p><p>③ <strong>数据采样：</strong>根据多模态特征和多模态标签<strong>检索给定样本的相似样本，从而在样本间执行有监督的对比学习</strong>。对于 D 中的每个样本对( i , j )，计算它们之间的余弦相似度得分（内积）。然后，检索每个样本的候选相似&#x2F;相异样本集。对于每个样本 i ，根据相似度得分从高到低将具有相同多模态标签$y_m^i$的样本排序，作为候选相似样本集 $S_0^i$。将标签$y_m^i$ 以外的样本排序，作为候选相异样本集 $S_1^i$。</p><p>从候选相似样本集$S_0^i$ 中随机选取两个余弦相似度得分较高的近似样本，与样本 i构成样本间正对，记为$\text{Neighbor}^i$；从候选相异样本集 $S_1^i$中随机选取四个不同的样本组成样本间负对，记为$\text{Outlier}^i$（其中两个样本具有较低的余弦相似度分数$$\text{Outlier}^i_1$$ ，另外两个样本具有较高的余弦相似度分数$$\text{Outlier}^i_2$$）。</p><p><strong>为什么这样选择？</strong>通常倾向于选择$\text{Neighbor}^i$与$$\text{Outlier}^i_1$$ 中的样本分别与样本 i 形成正对和负对。但是由于$\text{Outlier}^i_2$中的样本与样本 i  具有不同的标签，并且具有相似的语义信息，所以$\text{Outlier}^i_2$中的样本很难与样本 i 区分开。故，将$\text{Outlier}^i_2$中的样本也添加到$\text{Outlier}^i$中。通过对比学习让$\text{Outlier}^i_2$中的样本与样本 i 区分开。</p><p>④<strong>样本内intra-sample的正对&#x2F;负对构建：</strong>使用六个分解特征形成样本内的正&#x2F;负对。选择文本相似性特征 $T_s^i$ 作为锚点，使视觉和听觉相似性特征 $A_s^i$ 与 $V_s^i$ 向 $T_s^i$ 靠拢，同时将所有模态中的相异性特征 $T_d^i$ , $V_d^i$ , $A_d^i$ 远离  $T_s^i$ 。</p><p><img src="F:/files/myBlog/images/image-20241011161442040.png" alt="image-20241011161442040"></p><p>注意：$\text{Neighbor}^i$与$\text{Outlier}^i$ 分别代表样本 i 的相似样本和相异样本，$$<br>j \in \text { Neighbor }^i \cup \text { Outlier }^i<br>$$所起的作用是扩大对比范围。</p><p>⑤<strong>样本间inter-sample的正对&#x2F;负对构建：</strong>根据数据采样器采样得到的$\text{Neighbor}^i$与$\text{Outlier}^i$ 构建样本间的正对&#x2F;负对。</p><p><img src="F:/files/myBlog/images/image-20241011162146497.png" alt="image-20241011162146497"></p><p>注意：只使用相似性特征来获得样本间的配对，因为同一类别中相似性样本的相似性特征应该很接近，而不同类别中样本的相似性特征应该相距较远。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;在没有单模态标签的情况下，模型效果表现不好，因为在单模态预测任务中，是用多模态的标签监督学习，会起到干扰的作用</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;CMU-MOSI，CMU-MOSEI，CH-SIMS（带有单模态标签）</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;MOSI和MOSEI 结果不是很高，SIMS还行</p><p><img src="F:/files/myBlog/images/image-20241011162737378.png" alt="image-20241011162737378"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/XpastaX/ConFEDE">https://github.com/XpastaX/ConFEDE</a></p><h5 id="（2）Multi-level-Contrastive-Learning-Hierarchical-Alleviation-of-Heterogeneity-in-Multimodal-Sentiment-Analysis（IEEE-Transactions-on-Affective-Computing-2024）"><a href="#（2）Multi-level-Contrastive-Learning-Hierarchical-Alleviation-of-Heterogeneity-in-Multimodal-Sentiment-Analysis（IEEE-Transactions-on-Affective-Computing-2024）" class="headerlink" title="（2）Multi-level Contrastive Learning: Hierarchical Alleviation of Heterogeneity in Multimodal Sentiment Analysis（IEEE Transactions on Affective Computing 2024）"></a>（2）Multi-level Contrastive Learning: Hierarchical Alleviation of Heterogeneity in Multimodal Sentiment Analysis（IEEE Transactions on Affective Computing 2024）</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对比学习；多层级融合</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;现有方法比较少考虑不同的模态特征存在<strong>异构性</strong>的问题，这会影响后续的融合过程。模态的异构性是由模态之间<strong>不同的特征分布和不同的表征空间</strong>导致的。比如现有的方法通过注意力来融合，但不考虑异构性问题的话，数据分布之间的不一致会导致不平均的注意力权重分配。异构性也来自于不同模态组合，比如双模态和单模态之间的信息差异。<strong>而对比学习可以解决异构性问题，因为它可以把不同模态的数据投射到同一特征空间。</strong></p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;文章主要针对的点是异质性问题出现在模态融合的各个阶段：不同模态（文本、音频、视觉）的低层次特征表现（如纹理、语音音高，basic information）和高层次的语义特征（semantic information features）。所以在设计模型的时候<strong>把对比学习用在了三个阶段</strong>（渐进式融合，也是比较自然的想法），并且融合的过程也是分两步走。框架图如下，在first-level，second-level，third-level层都分别用到了对比学习。</p><p><img src="F:/files/myBlog/images/image-20240928165118509-1728702396078-1728702401197.png" alt="image-20240928165118509"></p><p>如框架图所示：</p><ul><li>第一层对比学习在多模态特征融合的早期阶段缓解了单模态之间的异质性。</li></ul><p><img src="F:/files/myBlog/images/image-20241012113714745.png" alt="image-20241012113714745"></p><ul><li>第二层对比学习减轻了单模态和融合模态之间的异质性。</li></ul><p><img src="F:/files/myBlog/images/image-20241012113921820.png" alt="image-20241012113921820"></p><ul><li>在第三层上，引入了张量卷积融合模块，该模块从融合的模态中提取高层语义特征，并通过对比学习在更高的特征层面缓解异构性。</li></ul><p><img src="F:/files/myBlog/images/image-20241012114140724.png" alt="image-20241012114140724"></p><p>对于融合过程，文章提出<strong>多层卷积融合（MCF）</strong>的方法模拟连续融合过程，多层对比学习和MCF可以协同操作，产生最优的融合结果。对于MCF模块，文章是受<strong>张量融合网络（TFN）</strong>的启发，TFN通过使用张量的外积将不同模态的数据融合在一起，形成一个高维的张量表示，这个张量表示捕获了不同模态间的相互作用。但由于这个张量表示含有冗余信息，因此作者改进引入多层的张量卷积融合。顾名思义，卷积融合就是用了卷积的操作来融合。不同于TFN的单次外积，MCF会先生成一个多模态“图像”（通过外积生成），然后使用二维卷积进行特征提取。这种方法可以像处理图像一样，多角度提取融合模态中的高级特征，因此达到逐步减少冗余信息的目的。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI,MOSEI,SIMS</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;挺高的</p><p><img src="F:/files/myBlog/images/image-20241012114225268.png" alt="image-20241012114225268"></p><p><img src="F:/files/myBlog/images/image-20241012114240460.png" alt="image-20241012114240460"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/Zhudogsi/MCL-MCF">https://github.com/Zhudogsi/MCL-MCF</a></p><h3 id="二、总结的问题（及对应的方法）"><a href="#二、总结的问题（及对应的方法）" class="headerlink" title="二、总结的问题（及对应的方法）"></a>二、总结的问题（及对应的方法）</h3><ol><li>不同模态表示信息不匹配&#x2F;矛盾：①学习模态的差异性unique和一致性common information，特征分解（multi-view）；②meta-learning去学习单模态标签。（讽刺识别任务，图像是开心的表情，可是文本是伤感的，当模态信息有冲突，如何挖掘出背后作者的情绪是积极&#x2F;消极的，这个对应就是 low-shared information的情况。）</li><li>异构性：减少分布差距。有mapping, translation, <strong>adversarial learning，对比学习</strong>方法。比如说对比学习方法，可以实现自监督，一是通过拉近同一样本不同模态（正样本）的距离，拉远不同样本不同模态（负样本）的距离，可以对齐不同模态的表示和拉大负样本间的差异性，从而学习到更加鲁棒的特征。二是可以使不同模态的数据投射到同一特征空间中，并在这个空间内保持模态间的紧密联系。这对于融合多模态特征、提高模态间的协同作用十分有帮助。</li><li>data imbalance问题：研究人员开始使用生成对抗学习来生成符合原始数据分布的新样本。具体来说，先前的工作通过最小化生成器和判别器学习到的数据分布来生成新的样本。（Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations–related work提到）</li><li>自适应权重：主要用到元学习，该策略主要是通过一小部分带有单模态标签的手工标注数据来学习每个模态的权重，并动态调整这些权重以适应不同的情感表达。也就是说，对于每个sample，模型可以根据模态之间的差异性和模态缺失情况，自动调整不同模态的权重。比如说，当某个模态信息对情感预测贡献较大时，模型会为该模态分配更高的权重。</li></ol><h3 id="三、其他Tips"><a href="#三、其他Tips" class="headerlink" title="三、其他Tips"></a>三、其他Tips</h3><ol><li>自适应</li><li>分层次</li><li>有监督和无监督</li><li>翻译网络：用来在图像和文本模态之间进行信息转换。比如说，通过翻译网络将图像模态的信息转换为文本模态的表示。这个翻译网络的目标是捕捉模态间共享的情感信息，特别是在一种模态缺失的情况下，能够利用另一种模态的翻译表示来替代缺失模态的数据。</li><li>学单模态标签：使用多模态标签来训练单模态信号可能会引入噪声，影响模型的性能。不同模态可能传达不同的情感方面，导致多模态样本中各模态之间的标签不一致，进一步加剧了噪声标签问题。</li><li>对抗学习分阶段，或者是突出文本的重要性，或者是自适应模态的权重</li></ol>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MSA </tag>
            
            <tag> 对比学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调研MSA中的方法（三）：元学习</title>
      <link href="/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%85%83%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>这一章介绍四篇MSA中经典的结合元学习方法的文章~本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="3-meta-learning"><a href="#3-meta-learning" class="headerlink" title="3. meta-learning"></a>3. meta-learning</h4><h5 id="（1）Meta-Learn-Unimodal-Signals-with-Weak-Supervision-for-Multimodal-Sentiment-Analysis-（arXiv-2024）"><a href="#（1）Meta-Learn-Unimodal-Signals-with-Weak-Supervision-for-Multimodal-Sentiment-Analysis-（arXiv-2024）" class="headerlink" title="（1）Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis （arXiv 2024）"></a>（1）Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis （arXiv 2024）</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;元学习，单模态标签学习，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;noisy label problem: 单模态表征学习如果用多模态的标签训练，会影响单模态的学习</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;The first to introduce metalearning  into MSA for learning accurate unimodal labels</p><p>主要思想是通过弱监督学习单模态标签，以改进多模态情感分析。提出框架Meta Uni-label Generation, MUG。MUG框架包括三个主要阶段：<strong>单模态网络的构建、多模态框架的预训练和元学习策略的应用</strong>。</p><p>①预训练：设计了基于对比学习的投影模块（Contrastive-based Projection Module, CPM），以缩小单模态和多模态表示之间的差距。具体来说，将多模态表示投影到单模态嵌入空间，并使用投影的多模态嵌入训练单模态预测器。通过对比学习提高单模态和多模态表示之间的互信息，减少它们之间的分布差距。预训练阶段图如下：</p><p><img src="F:/files/myBlog/images/image-20241011211004154.png" alt="image-20241011211004154"></p><p><img src="F:/files/myBlog/images/image-20241011211253416.png" alt="image-20241011211253416"></p><p>②元单标签校正网络（Meta Uni-label Correction Network, MUCN）的元学习过程：包括单模态去噪任务和多模态去噪任务。</p><ul><li>在元训练阶段（unimodal denoising task），训练MUCN以去噪手动损坏的多模态标签，并恢复原始多模态标签。通过高斯噪声防止MUCN学习身份映射，并提供模型学习最优单模态标签的能力。</li><li>在元测试阶段（multimodal denoising task），利用干净的多模态标签和表示指导MUCN的学习。通过估计MUCN是否能恢复干净的多模态标签来评估其有效性。</li><li>采用双层优化策略以提高MUCN的训练效果：如果MUCN在单模态去噪任务的参数在多模态去噪任务上表现的好，那就用原的这个参数来更新；如果表现不好，那就得加上利用多模态去噪任务的损失来进行参数更新。</li><li>单模态去噪和多模态去噪两个任务的区别，本质都是要恢复加了噪声的多模态标签y：<ul><li>单模态去噪：输入是$x_m$和加了高斯的多模态真实标签y，想要输出尽可能靠近真实标签y</li><li>多模态去噪：输入是利用统一多模态表示x映射后的 $x_m\prime$ 和加了高斯的单模态预测标签 $\hat{y}_m\prime$ （利用了预训练阶段的Projection Layer和Unimodal Predictor），想要输出尽可能靠近真实标签 y。</li></ul></li></ul><p><img src="F:/files/myBlog/images/image-20241011211441814.png" alt="image-20241011211441814"></p><p>③多任务训练：联合训练单模态和多模态学习任务，以提取更具辨别力的单模态特征。具体是单模态任务训练使用元学习阶段生成的校正单模态标签进行单模态任务训练；多模态任务训练使用多模态标签指导多模态任务学习。</p><p>④总结：通过这种方式，MUG能够最大限度地利用现有的多模态信息，提取更具辨别力和表达力的单模态特征，从而提高多模态情感分析的性能。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;CMU-MOSI；CMU-MOSEI；SIMS（中文数据集）</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20240930112043696-1728651971309.png" alt="image-20240930112043696"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;无</p><h5 id="（2）Learning-to-Learn-Better-Unimodal-Representations-via-Adaptive-Multimodal-Meta-Learning-TAC2023"><a href="#（2）Learning-to-Learn-Better-Unimodal-Representations-via-Adaptive-Multimodal-Meta-Learning-TAC2023" class="headerlink" title="（2）Learning to Learn Better Unimodal Representations via Adaptive Multimodal Meta-Learning(TAC2023)"></a>（2）Learning to Learn Better Unimodal Representations via Adaptive Multimodal Meta-Learning(TAC2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;元学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;解决以往多模态融合只能获得次优的单模态表征；缩小各模态之间的分布差异</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012102129511.png" alt="image-20241012102129511"></p><p><strong>①Distribution Transformation Layer（DTL）：</strong>减小模态特征分布之间的间隙，促进多模态融合。我们首先将单模态表示的分布转换为高斯分布，然后将转换后的分布在不同模态之间进行匹配。单模态表示转为高斯分布过程采用参数化技巧，以避免梯度计算过程中的随机性：</p><p><img src="F:/files/myBlog/images/image-20241012101742141.png" alt="image-20241012101742141"></p><p>通过KL散度 Loss来缩小gap：</p><p><img src="F:/files/myBlog/images/image-20241012101809280.png" alt="image-20241012101809280"></p><p><strong>②Adaptive Multimodal Meta-Learning：</strong></p><p>a. 元训练阶段（“Inner-Update Stage”）：为每种模态分配特定的元学习优化程序，从而获得每种模态的最佳的单模态表征，并将其用于多模态融合。与之前读的meta learning学习权重不同的是，这里学习的是学习率，控制更新方向和速度（归根到底还是Loss的设置和训练的方式和阶段不一样）：</p><p><img src="F:/files/myBlog/images/image-20241012101926713.png" alt="image-20241012101926713"></p><p>单模态表征学习网络的参数在inner-update and outer-update stages被更新两次。</p><p>b.元测试阶段（“Outer-Update Stage”）：</p><p><img src="F:/files/myBlog/images/image-20241012102020083.png" alt="image-20241012102020083"></p><p>这里的元学习指多模态任务信息指导单模态网络的更新。与传统meta learning任务不同的是，传统meta的元训练和元测试阶段数据不同，但setting相同。而这篇文章是数据相同，setting不同。我们的目标是将在元训练(单模态)任务中优化的unimodal learners适应到元测试(多模态)任务中，其中两个任务的目标是不同的。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;CMU-MOSEI and CMUMOSI</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;不高</p><p><img src="F:/files/myBlog/images/image-20241012102625880.png" alt="image-20241012102625880"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;无</p><h5 id="（3）Crossmodal-Translation-based-Meta-Weight-Adaption-for-Robust-Image-Text-Sentiment-Analysis-ACM-MM2021"><a href="#（3）Crossmodal-Translation-based-Meta-Weight-Adaption-for-Robust-Image-Text-Sentiment-Analysis-ACM-MM2021" class="headerlink" title="（3）Crossmodal Translation based Meta Weight Adaption for Robust Image-Text Sentiment Analysis (ACM MM2021)"></a>（3）Crossmodal Translation based Meta Weight Adaption for Robust Image-Text Sentiment Analysis (ACM MM2021)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;翻译网络，元学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;模态独特性的重要性，missing modality</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p>①跨模态翻译网络：用来在图像和文本模态之间进行信息转换。比如说，通过翻译网络将图像模态的信息转换为文本模态的表示。这个翻译网络的目标是捕捉模态间共享的情感信息，特别是在一种模态缺失的情况下，能够利用另一种模态的翻译表示来替代缺失模态的数据。</p><p><img src="F:/files/myBlog/images/image-20241012103210831.png" alt="image-20241012103210831"></p><p>在原始数据集手动给一些样本标注单模态的标签，标记为$D_{meta}$，剩下的数据集是$D_{train}$。</p><p>②普通优化pseudo-updating of the backbone network $\theta$：**在第$\tau$步，使用$D_{train}$，首先前向计算backbone网络$\Phi_b(\cdot;\theta^{(\tau)})$，网络输入输出如下：</p><p><img src="F:/files/myBlog/images/image-20241012103541059.png" alt="image-20241012103541059"></p><p>然后还要计算unimodal weight generation network ，网络输出的是每个样本的单模态权重：</p><p><img src="F:/files/myBlog/images/image-20241012103600142.png" alt="image-20241012103600142"></p><p>这一阶段的训练目标$L_1$如下：</p><p><img src="F:/files/myBlog/images/image-20241012103620308.png" alt="image-20241012103620308"></p><p>这里的loss，第一部分$L_c$是交叉熵，第二部分Σ项是权重比例乘以交叉熵项，与第一项交叉熵比较真实标签y和模型预测样本标签 $\hat{y}$ 不同的是，这一项比较的是真实标签y和单模态标签$\hat{y}_u$，也就是说这里的单模态标签会尽可能往总的标签预测，用统一的多模态label来监督单模态情感分析网络的学习，这样做的问题是会导致独立的单模态信息的扰动。最后一项 $L_r$ 。最后一项$L_r$是crossmodal translation网络的Loss值。</p><p>用这个$L_1$来更新旧网络参数$\theta^{(\tau)}$，得到新网络参数$\hat{\theta}^{(\tau)}$：</p><p><img src="F:/files/myBlog/images/image-20241012103641734.png" alt="image-20241012103641734"></p><p>总的流程图如下：</p><p><img src="F:/files/myBlog/images/image-20241012103712230.png" alt="image-20241012103712230"></p><p>可以看到，前向计算涉及backbone和UWG网络，后向更新参数是作用在backbone。</p><p>③元学习指导refining the unimodal weight generation network $\omega$：这一阶段主要的目标是给UWG网络的更新提供指导。因为前面第一阶段是用统一的多模态label来监督单模态情感分析网络的学习<strong>（bad）</strong>，这里引用了元学习的指导，使用人工标注好的单模态标签数据集$D_{meta}$。</p><p>训练目标如下：<img src="F:/files/myBlog/images/image-20241012103803300.png" alt="image-20241012103803300"></p><p>这里第二项Σ就是用的真实单模态标签$y_u$（人工标注）和预测标签$\hat{y_u}$。<strong>（good）</strong>不仅如此，这个Loss与第一阶段更新相比，没有translation network的$L_r$。</p><p>参数更新UWG网络：<img src="F:/files/myBlog/images/image-20241012103910297.png" alt="image-20241012103910297"></p><p><img src="F:/files/myBlog/images/image-20241012103936424.png" alt="image-20241012103936424"></p><p>可以看到，前向计算涉及backbone，后向更新参数是作用在UWG网络。</p><p>④使用 更新的UWG网络 得到的 单模态权重 来更新backbone $\theta$：这一个阶段的训练目标如下：</p><p> <img src="F:/files/myBlog/images/image-20241012104013814.png" alt="image-20241012104013814"></p><p>与第二阶段相比，这里的Σ项多了第二个阶段更新后的权重。计算与第一个阶段的训练目标是一样的，这不过这一阶段经历了第二阶段的人工单模态标签的指导，$\hat{y}_u$会更准确一些。</p><p>参数更新backbone：<img src="F:/files/myBlog/images/image-20241012104028870.png" alt="image-20241012104028870"></p><p><img src="F:/files/myBlog/images/image-20241012104059546.png" alt="image-20241012104059546"></p><p>可以看到，前向计算涉及backbone和UWG网络，后向更新参数是作用在backbone。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;①数据集可以从图文两模态拓展到视频三模态等，该文章模型的设计是需要单个模态的标签的，后续拓展的数据集中的单模态标签可以是自动生成的，应用范围可以更加广泛。②文章是用了跨模态翻译的方法处理模态缺失问题，之前有泛泛地了解到对抗学习也被用于处理模态缺失，对抗学习较于前者是侧重于生成更具逼真性的模态补全表示。具体来说，生成器负责生成缺失模态，判别器负责判断生成模态的真实性，通过对抗学习提升模态补全的质量。模型可以生成缺失模态的补全数据，从而提升模型的性能。具体模型有生成对抗网络（GAN）。③元学习在这篇文章的作用有些类似于之前看到的用多任务学习来辅助多模态情感分析任务，也是借助外部的信息（另一个任务或者另一些样本的信息）去辅助本身的任务。但两者对比，多任务学习更多是学习不同任务间共享的有效信息（因此要求不同任务之间是得有关联的），而本文提出的元学习策略倾向于捕获不同模态之间独有的信息，并且元学习更加轻量化，需要的样本数更少，还可以快速适应新任务。后续会调研一下，尝试将对抗学习与元学习结合，来进一步提高模型的稳健性。</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;image text sentiment analysis</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MVSA-Single and MVSA-Multiple，TumEmo</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012104300919.png" alt="image-20241012104300919"></p><p><img src="F:/files/myBlog/images/image-20241012104308536.png" alt="image-20241012104308536"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D; <a href="https://github.com/thuiar/CTMWA">https://github.com/thuiar/CTMWA</a></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MSA </tag>
            
            <tag> 元学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调研MSA中的方法（二）：对抗学习</title>
      <link href="/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>这一章介绍四篇MSA中经典的结合对抗学习方法的文章~本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="2-对抗学习"><a href="#2-对抗学习" class="headerlink" title="2. 对抗学习"></a>2. 对抗学习</h4><h5 id="（1）Supervised-Adversarial-Contrastive-Learning-for-Emotion-Recognition-in-Conversations-ACL2023"><a href="#（1）Supervised-Adversarial-Contrastive-Learning-for-Emotion-Recognition-in-Conversations-ACL2023" class="headerlink" title="（1）Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations (ACL2023)"></a>（1）Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations (ACL2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对抗学习，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;在label-based contrastive learning中，通过捕获类内示例之间的相似性，并与其他类中的示例进行对比，学习一个泛化的表示。这样做是因为相似的情感往往具有相似的上下文和重叠的特征空间。这些直接压缩每个类的特征空间的技术很可能会伤害每个情感的细粒度特征，从而限制泛化能力。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p>①首先，在原始样本上执行CAT策略和SCL目标，得到“contrast-aware adversarial perturbations”。在依赖上下文的对话场景中，直接生成对抗样本会干扰语句之间的相关性，不利于上下文理解。因此，本文提出“contextual adversarial training (CAT) strategy”，用来<strong>自适应地生成与上下文有关的worst-case samples</strong>，并从上下文中提取更多样化的特征：</p><p><img src="F:/files/myBlog/images/image-20241011200245367.png" alt="image-20241011200245367"></p><p>内层max部分让模型最大化失误，找到好的对抗扰动（最能欺骗模型，寻找让模型最脆弱的输入），外层min部分最小化损失，提高模型鲁棒性。</p><p><img src="data:image/png;base64,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" alt="img"></p><p>g是样本u的梯度更新方向，g提供了扰动方向，$\epsilon$ 是扰动的大小，确保$r_{c-adv}$ 大小在epsilon之内。归一化后的梯度 $\frac{g}{|g|_q}$  是一个长度为 1 的向量，保持与原始梯度 g 相同的方向，但大小（范数）被缩放为 1。这一步的目的是确保扰动的方向与梯度方向一致，但扰动的大小是受限的（通过乘以 $\epsilon$  控制），从而符合扰动的范数约束条件。</p><p>②接着，<strong>把这个perturbation放在hidden layer上</strong>，得到“hard positive examples”。</p><p><img src="F:/files/myBlog/images/image-20241011194600472.png" alt="image-20241011194600472"></p><p>③我们在获得的对抗样本上，以及原始样本上，都使用一种软SCL来最大化具有相同标签的样本表示的一致性。在这个联合目标下，该网络可以有效地学习标签一致的特征，并实现更好的泛化：</p><p><img src="F:/files/myBlog/images/image-20241011201456919.png" alt="image-20241011201456919"></p><p><img src="F:/files/myBlog/images/image-20241011202105302.png" alt="image-20241011202105302"></p><p>④总结：本文方法SACL与其他方法比较：有对抗样本，相对CL也比较soft</p><p><img src="F:/files/myBlog/images/image-20241011172018085.png" alt="image-20241011172018085"></p><p>从下图也可以看出模型的训练过程：左边是original samples，不同类之间pull apart，相同类之间pull closer，original samples产生对抗扰动，放到双向LSTM内部，产生对抗样本，对抗样本也是一样soft-SCL学习。原始样本和对抗样本有一个对抗的学习过程。</p><p><img src="F:/files/myBlog/images/image-20241011195052638.png" alt="image-20241011195052638"></p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;ERC</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;IEMOCAP，MELD，EmoryNLP（多人对话，用LSTM来提取特征party和utterance）</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241011170951060.png" alt="image-20241011170951060"></p><p><img src="F:/files/myBlog/images/image-20241011171003626.png" alt="image-20241011171003626"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/zerohd4869/SACL">https://github.com/zerohd4869/SACL</a></p><h5 id="（2）VLATTACK-Multimodal-Adversarial-Attacks-on-Vision-Language-Tasks-via-Pre-trained-Models-NeurIPS-2023"><a href="#（2）VLATTACK-Multimodal-Adversarial-Attacks-on-Vision-Language-Tasks-via-Pre-trained-Models-NeurIPS-2023" class="headerlink" title="（2）VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models (NeurIPS 2023)"></a>（2）VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models (NeurIPS 2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对抗攻击，预训练模型</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;现有的在VL任务中进行对抗性攻击的工作主要是在白盒设置下，攻击者可以访问微调模型的梯度信息。然而，在更现实的场景中，恶意攻击者可能只能访问通过第三方发布的公共预训练模型。攻击者不会对在私有数据集上微调的下游 VL 模型学习的参数有任何先验知识。为了弥补这一显着的局限性，我们研究了一种新的但实用的攻击范式——在预训练的 VL 模型上生成对抗性扰动，以攻击在预训练模型上微调的各种黑盒下游任务。</p><p>然而，这样的攻击设置并不简单，面临以下挑战： (1)特定任务的挑战。预训练的VL模型通常用于微调不同的下游任务，这要求<strong>设计的攻击机制具有通用性并且能够攻击多个任务</strong>。 (2) 特定于模型的挑战。由于微调模型的参数未知，因此需要<strong>攻击方法自动学习不同模态的预训练模型和微调模型之间的对抗性可转移性</strong>。尽管跨图像模型的对抗性可转移性已经被广泛讨论，但<strong>在预训练模型中仍然很大程度上未被探索</strong>，特别是<strong>在不同模态上的扰动之间构建相互联系</strong>。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;提出了一种新的通用视觉语言攻击策略（称为 VLATTACK），以探索预训练和微调的 VL 模型之间的对抗可迁移性。整个 VLATTACK 方案融合了扰动两个层次的图像和文本：</p><p>①单模态层面<br>VLATTACK 在单一模态上独立生成扰动，遵循“从图像到文本”的顺序，因为前者可以在连续空间上扰动。单模态攻击可以有效地检测图像或文本的对抗性漏洞，从而避免对其他模态的冗余扰动。具体来说，为了充分利用预训练模型中存储的图像文本交互，我们提出了一种新颖的<strong>逐块相似性攻击（BSA）策略来攻击图像模态</strong>，该策略添加扰动以<strong>逐块扩大网络预训练模型中原始特征和扰动特征之间的距离</strong>，破坏了下游预测的通用图像文本表示。如果BSA在查询微调的黑盒模型后未能改变预测，VLATTACK将通过采用词级扰动技术来攻击文本模态。我们采用 <strong>BERT-Attack来攻击文本模态</strong>，因为它的突出性能已在许多研究中得到广泛验证。最后，如果所有文本扰动 {T′ i} 失败，VLATTACK 将生成扰动样本 T 的列表，并将它们与扰动图像 I′ 一起馈送到多模态攻击。</p><p>②多模态层面<br>如果上述攻击未能改变预测，我们将根据先前的输出在<strong>多模态级别交叉更新图像和文本扰动</strong>。所提出的迭代交叉搜索攻击（ICSA）策略通过考虑不同模态扰动之间的相互关系，以迭代方式更新图像文本扰动对（I′ i，T′ i）。 ICSA 使用从列表 T 中选择的文本扰动 T′ i 作为指导，通过采用逐块相似性攻击 (BSA) 迭代更新扰动图像 I′ i，直到新的对 (I′ i, T′ i) 使得下游任务变化的预测。此外，在多模态攻击层面，根据与良性扰动的语义相似度对文本扰动进行交叉搜索，逐渐加大方向修改的程度，以最大程度地保留原始语义。</p><p>③图像模块攻击：</p><p><img src="F:/files/myBlog/images/image-20241011205705613.png" alt="image-20241011205705613"></p><p><img src="F:/files/myBlog/images/image-20241011205716281.png" alt="image-20241011205716281"></p><h5 id="（3）Adversarial-alignment-and-graph-fusion-via-information-bottleneck-for-multimodal-emotion-recognition-in-conversations-Information-Fusion2024"><a href="#（3）Adversarial-alignment-and-graph-fusion-via-information-bottleneck-for-multimodal-emotion-recognition-in-conversations-Information-Fusion2024" class="headerlink" title="（3）Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations (Information Fusion2024)"></a>（3）Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations (Information Fusion2024)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对抗学习，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;解决异质性问题</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012141927336.png" alt="image-20241012141927336"></p><p>①三个模态分别通过MLP映射得到初步的特征</p><p>②然后，我们构建了一个文本生成器和一个文本判别器。文本生成器“generator”的输入为音频特征ξ a和视频特征ξ v。文本判别器“discriminator”的输入是文本生成器生成的包含三种模态信息的融合特征。它们的loss学习目标（来源于博弈论的<strong>极小极大博弈</strong>）：</p><p><img src="F:/files/myBlog/images/image-20241012135934457.png" alt="image-20241012135934457"></p><ul><li>第一个是生成器的损失函数：生成器 Gt 的目标是生成高质量的文本使得判别器 Dt 无法分辨它们是真实的还是生成的。在设计上，生成器希望判别器对其生成的数据  给出尽可能接近1的判断值（即判别器认为它们是真实的）。所以生成器的损失函数是最小化 $log(1 - D_t(\cdot))$ ，代表生成器试图让判别器输出 $D_t(\cdot)$ 尽可能接近1。</li><li>第二个是判别器的损失函数：判别器 Dt 的任务是正确区分真实数据 T 和生成器生成的假数据 $G_t(\tilde{\xi_a})$  和 $G_t(\tilde{\xi_v})$ 。在设计上，判别器希望最大化其对真实数据的判断为“真”（即 $D_t(T)$  越接近1越好），同时希望最小化对生成数据的误判（即 $D_t(G_t(\tilde{\xi_a}))$  和 $D_t(G_t(\tilde{\xi_v}))$  应该尽量接近0）。</li></ul><p>同理再分别构建音频和视觉的生成器和判别器。</p><p>③IMCL: Intra-modal and inter-modal contrastive learning via IB</p><p>正样本：同一类别的同一模态的样本</p><p>负样本：同一类别的不同模态的样本</p><p>这里有一个问题就是，anchor与负样本的距离有可能非常大（相似度为0），这样正样本之间的距离对18公式的loss计算就不太影响了（有时候它会分得太过“远”，而忽略了一些细微的相似性），因为模型已经趋于最优了，因此加了一个约束项，负样本的相似度不能太小，要往β值靠近：</p><p><img src="F:/files/myBlog/images/image-20241012142322838.png" alt="image-20241012142322838"></p><p>④ICCL: Intra-class and Inter-class Contrastive Learning via IB：正负样本应该都是同一模态下，正样本是同一类别，负样本是不同类别。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;文章说对比学习需要计算最小化views的互信息，利用IB 来计算。但IB部分没怎么讲</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;Multimodal Emotion Recognition in Conversation</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MELD，IEMOCAP</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012142803496.png" alt="image-20241012142803496"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/yuntaoshou/AGF-IB">https://github.com/yuntaoshou/AGF-IB</a></p><h5 id="（4）Modality-to-Modality-Translation-An-Adversarial-Representation-Learning-and-Graph-Fusion-Network-for-Multimodal-Fusion-AAAI2020"><a href="#（4）Modality-to-Modality-Translation-An-Adversarial-Representation-Learning-and-Graph-Fusion-Network-for-Multimodal-Fusion-AAAI2020" class="headerlink" title="（4）Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion (AAAI2020)"></a>（4）Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion (AAAI2020)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;翻译网络，对抗学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;modality gap</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012144152727.png" alt="image-20241012144152727"></p><p>本文提出了<strong>新颖的对抗编码解码器来学习模态无关的嵌入空</strong>间，由于不同模态的分布（distribution）可能是不一样的，所以使用对抗训练的方法，根据编码器将源模态的分布转为目标模态的分布。不仅如此，还引入了重构损失以及分类损失对嵌入空间施加约束。当学习到表示之后，就是对于表示进行融合。本文使用了<strong>分层的图神经网络</strong>以多阶段的方法对于单模态，双模态和三模态的交互进行建模。</p><p>①Joint Embedding Space Learning</p><p>modality2modality translation：将源模态的分布转换为目标模态的分布以此可以得到一个模态无关的嵌入空间</p><p>模态的分布转换是这样的：通过优化参数$\theta_m$，来得到转换后的模态表征在学习嵌入空间中的分布$p_{\theta_{m}}\left(x_{m}^{e}\right)$.</p><p><img src="F:/files/myBlog/images/image-20241012144529980.png" alt="image-20241012144529980"></p><p>然而，不同模态的分布十分复杂且性质各异，极难通过简单的编码网络进行匹配。因此，我们利用对抗训练为变换后的分布添加约束。具体来说，文中用到了模态传译方面的知识，由模态传递所引发产生一个对抗的编码框架，也就是说，生成器从单一模态特征中生成一个较好的编码表示，而判别器则需要判别这个表示是否是目标模态所生成的。</p><p>定义了一个判别器D，其目的是将 $p_{\theta_{l}}\left(x_{l}^{e}\right)$ 分类为真，而将$p_{\theta_{a}}\left(x_{a}^{e}\right)$和$p_{\theta_{v}}\left(x_{v}^{e}\right)$ 分类为假，而生成器(它们是编码器Ea和Ev)试图欺骗判别器D，将$p_{\theta_{a}}\left(x_{a}^{e}\right)$和$p_{\theta_{v}}\left(x_{v}^{e}\right)$分类为真。对抗学习的loss：</p><p> <img src="F:/files/myBlog/images/image-20241012144848967.png" alt="image-20241012144848967"></p><p>公式2，第一项是生成器的loss，让判别器尽可能判断生成的v和a模态的分布为真；第二项是判别器的loss，让判别器把 l 模态判别正确，其他模态（a，v）判别错误。</p><p>如果判别器不能从所有模态中分辨出目标模态文本模态($p_{\theta_{l}}\left(x_{l}^{e}\right)D\left(\boldsymbol{x}<em>{a}^{e}\right) \approx D\left(\boldsymbol{x}</em>{v}^{e}\right) \approx D\left(\boldsymbol{x}_{l}^{e}\right)$ )，则各种模态的分布被成功地映射到一个模态不变的嵌入空间中。所以可以通过对抗学习来缩小modality gap。</p><p>“Transforming distributions”可能导致挖掘模态间互补信息所需的单模态信息丢失，所以为了获得在学习的嵌入空间中的模态独特信息，定义了decoder，加上encoder其实就是一个auto-encoder：</p><p><img src="F:/files/myBlog/images/image-20241012145048498.png" alt="image-20241012145048498"></p><p>综上，loss一共是有四部分，有两个是对抗学习的loss，一个是自编码器的loss，另外一个是MSE loss</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI，MOSEI，IEMOCAP</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;不好</p><p><img src="F:/files/myBlog/images/image-20241012144410789.png" alt="image-20241012144410789"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/TmacMai/ARGF_multimodal_fusion">https://github.com/TmacMai/ARGF_multimodal_fusion</a></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MSA </tag>
            
            <tag> 多视图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调研MSA中的方法（一）：Multi-view</title>
      <link href="/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AMulti-view/"/>
      <url>/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AMulti-view/</url>
      
        <content type="html"><![CDATA[<p>这一章介绍四篇MSA中经典的结合多视图方法的文章~预告本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="1-multi-view"><a href="#1-multi-view" class="headerlink" title="1. multi-view"></a>1. multi-view</h4><h5 id="（1）FACTORIZED-CONTRASTIVE-LEARNING-Going-Beyond-Multi-view-Redundancy-NeurIPS-2023"><a href="#（1）FACTORIZED-CONTRASTIVE-LEARNING-Going-Beyond-Multi-view-Redundancy-NeurIPS-2023" class="headerlink" title="（1）FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy (NeurIPS 2023)"></a>（1）FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy (NeurIPS 2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对比学习，重视unique information，</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;多视图学习有基础的假设“multi-view redundancy” ：模态之间贡献的信息都是与任务相关的。忽略了“多视图的非冗余和独特的信息可能是重要的”这种情况。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012150724734.png" alt="image-20241012150724734"></p><p><img src="F:/files/myBlog/images/image-20241012150707918.png" alt="image-20241012150707918"></p><p>数据增强：</p><p><img src="F:/files/myBlog/images/image-20241012150748519.png" alt="image-20241012150748519"></p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;①可以使用更好的MI上下界来优化目标，具体参考一些文献；</p><p>②目前的数据增强方法需要手动选择增强方式，以大致满足<strong>定义 4</strong> 中的要求。可以用其他方式替代：①<strong>自动生成数据增强</strong>：可以扩展 <strong>InfoMin</strong> 方法，使其能够自动生成数据增强策略，以更好地满足定义 4 的标准。这意味着，不再需要手动选择增强方法，而是可以通过算法自动决定哪些增强方式最有效。②利用未来的多模态生成模型。</p><p>③可以衡量shared 或unique information哪个信息对任务来说更重要，然后可以给优化目标函数的项赋不一样的权重值。</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;包括医疗，MSA，sarcasm任务</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MIMIC，MOSEI，MOSI，UR-FUNNY</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/pliang279/FactorCL">https://github.com/pliang279/FactorCL</a>.</p><h5 id="（2）Gacs-Korner-Common-Information-Variational-Autoencoder-NeurIPS-2023"><a href="#（2）Gacs-Korner-Common-Information-Variational-Autoencoder-NeurIPS-2023" class="headerlink" title="（2）Gács-Körner Common Information Variational Autoencoder (NeurIPS 2023)"></a>（2）Gács-Körner Common Information Variational Autoencoder (NeurIPS 2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;重视common information</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;假设用 (A, B, C) 来构造两个变量 X 和 Y，使$$X &#x3D; f(A, C)$$，$$Y &#x3D; g(B, C)$$，并且 C 编码了 X 和 Y 之间的所有且仅有的互信息 $$I(X; Y)$$，没有办法求得从高维数据中编码的这个最大公因数C。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;GK-VAE</p><p><img src="F:/files/myBlog/images/image-20241012151353797.png" alt="image-20241012151353797"></p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;图像分类</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;3dshapes，dsprites</p><p><img src="F:/files/myBlog/images/image-20241012151540576.png" alt="image-20241012151540576"></p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/mjkleinman/common-vae">https://github.com/mjkleinman/common-vae</a></p><h5 id="（3）MISA-Modality-Invariant-and-Specific-Representations-for-Multimodal-Sentiment-Analysis-ACM-MM2020"><a href="#（3）MISA-Modality-Invariant-and-Specific-Representations-for-Multimodal-Sentiment-Analysis-ACM-MM2020" class="headerlink" title="（3）MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis (ACM MM2020)"></a>（3）MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis (ACM MM2020)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;重视common和unique information</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;异构性导致的distributional modality gaps</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;文章主要focus在如何有效地学习模态表示上，核心idea是将每个模态映射到两个不同的子 空间（factorized representations）。在第一个子空间中，学习模态间的共同信息（commonalities &#x2F;  common information）来减少模态间的差异。在第二个子空间中，学习每个模态私有的、特有的信 息。</p><p><img src="F:/files/myBlog/images/image-20241012151825928.png" alt="image-20241012151825928"></p><p>主要用了四个loss来学习模态表示：相似度通过内积计算</p><img src="F:/files/myBlog/images/image-20240511225739028.png" alt="image-20240511225739028" style="zoom:80%;" /><p>结合框架图来看，不同的loss起着各自的作用：</p><ul><li>Similarity Loss（$\mathcal{L}_{sim}$）：通过最小化相似性损失，减少每个模态的共享表示之间的差异，有助于将共同的跨模态特征对齐在一起。</li><li>Difference Loss（$\mathcal{L}_{diff}$）：通过施加软正交约束来确保模态不变和特定表示捕获输入的不同方面，同时还添加了模态特定向量之间的正交性约束。</li><li>Reconstruction Loss（$\mathcal{L}_{recon}$）：通过添加重构损失来确保潜在特征捕获其各自模态的细节，避免学习到无关紧要的表示。</li><li>Task Loss（$\mathcal{L}_{task}$）：用于评估训练时模型预测任务的准确性，对于分类任务使用标准的交叉熵损失，对于回归任务使用均方误差损失。</li></ul><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;没有量化共有信息和unique信息</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA，sarcasm</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI，MOSEI，FUNNY</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;很一般</p><p><img src="F:/files/myBlog/images/image-20241012152356165.png" alt="image-20241012152356165"></p><p><img src="F:/files/myBlog/images/image-20241012152405408.png" alt="image-20241012152405408"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/declare-lab/MISA">https://github.com/declare-lab/MISA</a></p><h5 id="（4）Multi-View-Interactive-Representations-for-Multimodal-Sentiment-Analysis-IEEE-TRANSACTIONS-ON-CONSUMER-ELECTRONICS-2024"><a href="#（4）Multi-View-Interactive-Representations-for-Multimodal-Sentiment-Analysis-IEEE-TRANSACTIONS-ON-CONSUMER-ELECTRONICS-2024" class="headerlink" title="（4）Multi-View Interactive Representations for Multimodal Sentiment Analysis (IEEE TRANSACTIONS ON CONSUMER ELECTRONICS 2024)"></a>（4）Multi-View Interactive Representations for Multimodal Sentiment Analysis (IEEE TRANSACTIONS ON CONSUMER ELECTRONICS 2024)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;自监督标签生成</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;现有的多模态情感分析方法在捕捉不同交互状态下的多视角情感线索方面存在不足，导致多模态表示的表达能力受限。本文提出了一个新框架（MVIR），通过在多种交互状态下学习共享和私有的情感信息，增强了情感分析的表现。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;<strong>MVIR框架</strong>：一个创新的多模态情感分析框架，利用多任务学习来捕捉不同交互状态下的多视角交互表示。</p><p><strong>DVAWF算法</strong>：设计了双视角注意力（多头图注意力网络和多头自注意力机制）加权融合机制，促进跨模态的交互，增强特征融合。</p><p><strong>SSLGM模块</strong>：引入自监督标签生成模块，为不同交互状态生成伪标签，进一步优化模型的情感表示能力。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI, MOSEI, SIMS</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;一般</p><p><img src="F:/files/myBlog/images/image-20241012163328960.png" alt="image-20241012163328960"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;无</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MSA </tag>
            
            <tag> 多视图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记：Divide, Conquer and Combine Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing</title>
      <link href="/2023/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0Divide-Conquer-and-Combine-Hierarchical-Feature-Fusion-Network-with-Local-and-Global-Perspectives-for-Multimodal-Affective-Computing/"/>
      <url>/2023/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0Divide-Conquer-and-Combine-Hierarchical-Feature-Fusion-Network-with-Local-and-Global-Perspectives-for-Multimodal-Affective-Computing/</url>
      
        <content type="html"><![CDATA[<p>方法的核心正如题目所说，设计了一个分层的融合网络，去学习局部和全局的信息，并且作者把这个流程定为了三个阶段：分治合——Divide，Conquer，Combine。</p><span id="more"></span><p>总体框架图如下：文本、图像、语音各自提取特征，在Divide阶段加入了一个滑动窗口，相当于把三个模态的特征进行了“切割”；再对切割后的小块局部特征进行外积操作（跟TFN模型一样的思想），外积就是在融合局部特征，是Conquer阶段；最后在Combine阶段设计了ABS-LSTM，用于提取全局特征。</p><p><img src="/images/image-20231007190803453.png" alt="image-20231007190803453"></p><p>接下来具体讲一下模型部分的某些细节，顺便讲一下论文提出的ABS-LSTM。</p><p>之所以ABS-LSTM能提取全局特征，是因为他特别注意前t个时间步的交互和信息，在当前输入的情况下，对前t个时间步给予权重得分。</p><p>第<code>L</code>步状态的cell和states计算如下：</p><p><img src="/images/image-20231007193954999.png" alt="image-20231007193954999"></p><p>接着就是LSTM的计算公式：</p><p><img src="/images/image-20231007194913954.png" alt="image-20231007194913954"></p><p>与传统的LSTM相比，h<del>l</del>和c<del>l</del>替换掉了h<del>l-1</del>和c<del>l-1</del>，传统的LSTM的对之前信息的融入通过上一步的h<del>l-1</del>和c张量，但ABS-LSTM对以前信息直接focus在前t个时间状态中，这样当前步对它们的关注更多。</p><p>同时为了防止之前的信息被稀释掉，在LSTM的hidden states计算中加入了Attention机制GIA：</p><p><img src="/images/image-20231007202145251.png" alt="image-20231007202145251"></p><p>w<del>h</del>和w<del>x</del>这两个参数蕴含了第l步隐藏向量和当前输入的重要性。所以在h<del>l</del>^a^的计算中，W<del>h2</del>w<del>h</del>作为权重，W<del>x2</del>w<del>x</del>作为bias，融入了这两个信息。</p><p>结果：</p><p><img src="/images/image-20231007204539523.png" alt="image-20231007204539523"></p><p>这还有一个小点，文章对滑动窗口的步长s和大小d进行了分析，发现当s大于d，也就是当有一些模态特征在融合阶段被遗留下来的时候，效果并没有很大的变化，这可以推测出模态特征有冗余的部分。</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper Reading </tag>
            
            <tag> MSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记：UniMSE:
Towards Unified Multimodal Sentiment Analysis and Emotion Recognition
</title>
      <link href="/2023/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0UniMSE-Towards-Unified-Multimodal-Sentiment-Analysis-and-Emotion-Recognition/"/>
      <url>/2023/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0UniMSE-Towards-Unified-Multimodal-Sentiment-Analysis-and-Emotion-Recognition/</url>
      
        <content type="html"><![CDATA[<p>这篇文章有效结合了多模态情感分析和情绪识别两种任务。</p><p>发表出处：EMNLP 2022</p><span id="more"></span><p><del>第一篇给老师汇报的论文，现在再写下笔记，有点把呕了的东西再吃下去的感觉:joy:</del></p><p>笔记从下面五部分讲解：</p><h3 id="Research-Background"><a href="#Research-Background" class="headerlink" title="Research Background"></a>Research Background</h3><p><img src="/images/image-20231007163658519.png" alt="image-20231007163658519"></p><ul><li>从心理认知角度来看，情感分析MSA和情绪识别ERC的人类表达方式是一样的，所以这两个任务在直觉上可以进行关联和互补，但是现研究针对这两个任务的研究一般都是独立的，没有把他们关联起来。</li><li>情感Sentiment和情绪Emotion的定义：前者持续时间长，预测极性标签（positive，1.6）；后者持续时间短，预测情绪类别（joy）</li><li>&#x3D;&#x3D;Figure 1这张图初步展示了情感和情绪可以共享一个统一的表示空间，具体来说是通过具有相同的情感标签的样例的相似度来确定统一的一个lable&#x3D;&#x3D;</li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p><img src="/images/image-20231007164203289.png" alt="image-20231007164203289"></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>这部分的讲解可以分为五大部分：Method-Overall Architecture，Method-Task Formalization，Method-Pre-trained Modality Fusion(PMF) ，<strong>Method-Inter-modality Contrastive Learning</strong> ，Method- Grounding UL to MSA and ERC</p><ol><li><strong>Method-Overall Architecture</strong></li></ol><p><img src="/images/image-20231007164416156.png" alt="image-20231007164416156"></p><p><img src="/images/image-20231007164437307.png" alt="image-20231007164437307"></p><p>&#x3D;&#x3D;Figure 2有几个注意的点：训练集是都用到了MSA和ERC 的数据集，通过UL Lable集合起来。然后融合部分引进了对比学习，对比学习的过程就是以锚点为参照，在特征空间中将锚点和它的正样本拉得更近，将锚点和负样本推得更远。而在这个模型中具体来说就是以文本模态（小紫）为锚点，在同一个样例中三种模态让他们的表示相近（即pull close），不同的样例的三种模态push far，所以说这里的正样例有1个，负样例有n个&#x3D;&#x3D;</p><ol start="2"><li><strong>Method-Task Formalization</strong></li></ol><p><img src="/images/image-20231007164806498.png" alt="image-20231007164806498"></p><p>语音模态：用librosa 将原始声音输入处理成数值序列向量，提取梅尔谱图作为音频特征。</p><p>视频模态：从每个片段中提取固定的T帧，并使用在VGGface 4和AFEW数据集上预训练(有监督)的effecientNet来获取视频特征。</p><p><img src="/images/image-20231007164821984.png" alt="image-20231007164821984"></p><p>&#x3D;&#x3D;这里要注意他为什么能将MSA和ERC样本按照情感极性分为积极、中性和消极样本集。（我当时困惑了好久，以为ERC的数据集的标签只有情绪类别，哪来的情感极性，但其实这里用到的ERC数据集是有情感极性的，MSA数据集没有情感标签。大概是这样，具体可以看看代码和数据集。</p><ol start="3"><li><strong>Method-Pre-trained Modality Fusion(PMF)</strong></li></ol><p><img src="/images/image-20231007164854172.png" alt="image-20231007164854172"></p><p>M<del>i</del>为经过LSTM后得到的模态表征</p><ol start="4"><li><strong>Method-Inter-modality Contrastive Learning</strong></li></ol><p><img src="/images/image-20231007164908541.png" alt="image-20231007164908541"></p><ol start="5"><li><strong>Method- Grounding UL to MSA and ERC</strong></li></ol><p><img src="/images/image-20231007164948129.png" alt="image-20231007164948129"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/images/image-20231007165412671.png" alt="image-20231007165412671"></p><p>后两个数据集用ACC\WF1</p><p><img src="/images/image-20231007165435291.png" alt="image-20231007165435291"></p><p>效果嘎嘎好</p><p><img src="/images/image-20231007165449012.png" alt="image-20231007165449012"></p><p><img src="/images/image-20231007165500944.png" alt="image-20231007165500944"></p><h3 id="Conclusion-Limitations"><a href="#Conclusion-Limitations" class="headerlink" title="Conclusion&amp;Limitations"></a>Conclusion&amp;Limitations</h3><p><img src="/images/image-20231007165512816.png" alt="image-20231007165512816"></p><p><img src="/images/image-20231007165523868.png" alt="image-20231007165523868"></p><p>Limitation这里，样例相似度是通过计算文本模态的相似度得到的，没有用到其他模态，所以作者认为相似度这里还可以改进。</p><p>启发：特征的融合也许还可以改进，即这部分，主要还是用到拼接</p><p><img src="/images/image-20231007172822666.png" alt="image-20231007172822666"></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper Reading </tag>
            
            <tag> MSA </tag>
            
            <tag> Emotion Recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验室环境搭建</title>
      <link href="/2023/10/07/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2023/10/07/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p>实验室配备了新的电脑，故此记录实验室环境的配置。</p><span id="more"></span><h3 id="服务器连接"><a href="#服务器连接" class="headerlink" title="服务器连接"></a>服务器连接</h3><ol><li>创建服务器账号，需要先生成公钥（命令行生成</li><li>用mobaxterm连接服务器账号，填写好配置信息，连接成功<ul><li>这里出现一个问题，当时生成公钥配置了密码，一直permission denied（应该是设置权限问题，找Administrator</li></ul></li></ol><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>Anaconda+VSCode</p><p>选择原因：Anaconda内嵌了Python，而且可以管理环境；VSCode可以通过插件连接服务器（Pycharm要专业版收费:cry:</p><p>找了几个教程手把手操作，还在Anaconda分别新建了Pytorch和Tensorflow环境（注意cpu和gpu版本）。但是忘记保存下来了&#x3D;&#x3D;</p><h3 id="conda命令行"><a href="#conda命令行" class="headerlink" title="conda命令行"></a>conda命令行</h3><p><strong>创建虚拟环境</strong>：yida_cv是我虚拟环境的名字，你取什么名字都OK，最好能够标记好环境。<br><code>conda create -n yida_cv python=3.6 </code></p><p><strong>激活&#x2F;切换虚拟环境</strong><br><code>conda activate yida_cv</code></p><p><strong>退出并进入base环境</strong><br><code>conda deactivate </code></p><p><strong>查看已有的虚拟环境</strong><br><code>conda env list</code></p><p><strong>删除虚拟环境</strong><br><code>conda remove -n yida_cv --all</code></p><hr><p>参考更多命令行：<br><a href="https://blog.csdn.net/weixin_43312117/article/details/123431626">https://blog.csdn.net/weixin_43312117/article/details/123431626</a></p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的第一篇博客</title>
      <link href="/2023/10/05/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2023/10/05/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p>主要记录Linux命令~</p><span id="more"></span><h2 id="第一篇blog"><a href="#第一篇blog" class="headerlink" title="第一篇blog"></a>第一篇blog</h2><p>主要参考🔗：<a href="https://zhuanlan.zhihu.com/p/102592286">从零开始搭建个人博客（超详细） - 知乎 (zhihu.com)</a></p><p>感谢！</p><h3 id="使用git"><a href="#使用git" class="headerlink" title="使用git"></a>使用git</h3><p>在Blog根目录下git bash 进入</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>hexo s</td><td>server 写完博客可以先本地看看，再deploy</td></tr><tr><td>hexo clean</td><td>clean 不是每一次都需要执行</td></tr><tr><td>hexo g</td><td>generate</td></tr><tr><td>hexo d</td><td>deploy 部署</td></tr><tr><td>hexo new “My New Post”</td><td>新建Blog</td></tr></tbody></table><h3 id="使用Linux命令-vim用法"><a href="#使用Linux命令-vim用法" class="headerlink" title="使用Linux命令-vim用法"></a>使用Linux命令-vim用法</h3><p>在学习的过程中，发现有一些大佬用Linux读取文件写文件一顿操作猛如虎（respect，而我还只会打开文件资源管理器，</p><p>所以在此记录下一些关于文件的Linux命令的用法：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">##for example </span><br><span class="line">pwd       ##查看当前路径</span><br><span class="line">ls -l</span><br><span class="line">cd themes/</span><br><span class="line">cd ..     ##回到上层</span><br><span class="line">vim congfig.yml</span><br><span class="line"></span><br><span class="line">##输入内容</span><br><span class="line">i   ##insert，输入i后，即可修改文件内容</span><br><span class="line">exc    ##退回</span><br><span class="line"></span><br><span class="line">#保存，但不退出vi</span><br><span class="line">:w    ##注意：不可省略</span><br><span class="line"></span><br><span class="line">##保存并退出vi</span><br><span class="line">:wq</span><br><span class="line"></span><br><span class="line">##退出vi，但不保存更改 </span><br><span class="line">:q!</span><br><span class="line"></span><br><span class="line">##用其他文件名保存                        </span><br><span class="line">:w filename</span><br><span class="line"></span><br><span class="line">##在现有文件中保存并覆盖该文件    </span><br><span class="line">:w! filename</span><br><span class="line"></span><br><span class="line">##跳转</span><br><span class="line">shift + g  ## 到最后一行</span><br><span class="line">gg         ## 首行</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown语法</title>
      <link href="/2023/09/11/markdown%E8%AF%AD%E6%B3%95/"/>
      <url>/2023/09/11/markdown%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>让我们来快乐学起来Markdown吧~</p><span id="more"></span><p>reference：<a href="https://markdown.com.cn/basic-syntax/%EF%BC%8C">https://markdown.com.cn/basic-syntax/，</a>&lt;【8分钟让你快速掌握Markdown】 <a href="https://www.bilibili.com/video/BV1JA411h7Gw/?share_source=copy_web&vd_source=2c2cbf463a2a283764f665e8407d50a4%3E">https://www.bilibili.com/video/BV1JA411h7Gw/?share_source=copy_web&amp;vd_source=2c2cbf463a2a283764f665e8407d50a4&gt;</a></p><h2 id="1-标题前加井号"><a href="#1-标题前加井号" class="headerlink" title="1. 标题前加井号"></a><strong>1. 标题前加井号</strong></h2><p><em>PS:有六级</em></p><h3 id="小标题"><a href="#小标题" class="headerlink" title="小标题"></a>小标题</h3><h4 id="小小标题"><a href="#小小标题" class="headerlink" title="小小标题"></a>小小标题</h4><h2 id="2-粗体是两星号，斜体是一星号，下划线，高亮，emoji"><a href="#2-粗体是两星号，斜体是一星号，下划线，高亮，emoji" class="headerlink" title="2. 粗体是两星号，斜体是一星号，下划线，高亮，emoji"></a><strong>2. 粗体是两星号，斜体是一星号，下划线</strong>，高亮，emoji</h2><p>这里是<strong>加粗</strong><br>这里是<em>斜体</em><br>既<em><strong>加粗又斜体</strong></em></p><p><u>下划线</u></p><p>&#x3D;&#x3D;高亮文字&#x3D;&#x3D;</p><p>:smile::joy:</p><h2 id="3-引用是大于号，嵌套是多个大于号"><a href="#3-引用是大于号，嵌套是多个大于号" class="headerlink" title="3. 引用是大于号，嵌套是多个大于号"></a><strong>3. 引用是大于号，嵌套是多个大于号</strong></h2><blockquote><p>引用</p><blockquote><p>引用嵌套</p></blockquote></blockquote><h2 id="4-有序列表用数字，无序列表用横杠，嵌套按tab"><a href="#4-有序列表用数字，无序列表用横杠，嵌套按tab" class="headerlink" title="4. 有序列表用数字，无序列表用横杠，嵌套按tab"></a><strong>4. 有序列表用数字，无序列表用横杠，嵌套按tab</strong></h2><ol><li>有序列表</li><li>有序列表2</li></ol><ul><li>无序列表</li><li>无序列表2<ul><li>嵌套</li></ul></li></ul><h2 id="5-图片链接用中括号和圆括号"><a href="#5-图片链接用中括号和圆括号" class="headerlink" title="5. 图片链接用中括号和圆括号"></a><strong>5. 图片链接用中括号和圆括号</strong></h2><ul><li>一张网络图片<img src="//alifei04.cfp.cn/creative/vcg/veer/1600water/veer-303764513.jpg" alt="pic, test_picture" title="来自veer.com"></li><li>这是一个链接 <a href="https://markdown.com.cn/">Markdown语法</a></li><li>引用链接 <a href="https://markdown.com.cn/" title="markdown官网">Markdown语法</a>, <a href="https://markdown.com.cn/" title="markdown官网">Markdown语法</a>, <a href="https://markdown.com.cn/" title="markdown官网">Markdown语法</a></li><li>请参考[标题1](#1. 标题前加井号)</li><li>本地照片 <img src="/../images/background.png" alt="background"></li></ul><h2 id="6-代码用反引号"><a href="#6-代码用反引号" class="headerlink" title="6. 代码用反引号"></a><strong>6. 代码用反引号</strong></h2><p>行内代码实例 <code>nano</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><h2 id="7-数学公式用latex，上下标"><a href="#7-数学公式用latex，上下标" class="headerlink" title="7. 数学公式用latex，上下标"></a><strong>7. 数学公式用latex，上下标</strong></h2><p>$$<br>\frac[\partial f][\partial x]<br>$$</p><p>$\theta&#x3D;x^2$</p><p>H<del>2</del>O, X^2^</p><h2 id="8-转义符号"><a href="#8-转义符号" class="headerlink" title="8. 转义符号"></a><strong>8. 转义符号</strong></h2><p>在前面加转义符号\</p><h2 id="9-分隔符号是空一行加横线"><a href="#9-分隔符号是空一行加横线" class="headerlink" title="9. 分隔符号是空一行加横线"></a><strong>9. 分隔符号是空一行加横线</strong></h2><p>text1</p><hr><p>text2</p><hr><h2 id="10-任务列表用中括号"><a href="#10-任务列表用中括号" class="headerlink" title="10. 任务列表用中括号"></a><strong>10. 任务列表用中括号</strong></h2><p><em>PS:中括号里面和后面要空一格</em></p><ul><li><input checked="" disabled="" type="checkbox"> 事1</li><li><input disabled="" type="checkbox"> 事2</li></ul><h2 id="11-表格"><a href="#11-表格" class="headerlink" title="11. 表格"></a><strong>11. 表格</strong></h2><table><thead><tr><th align="left">name</th><th align="right">age</th><th align="center">grade</th></tr></thead><tbody><tr><td align="left">Taylor</td><td align="right">22</td><td align="center">98</td></tr><tr><td align="left">Mike</td><td align="right">22</td><td align="center">100</td></tr></tbody></table><h2 id="12-脚注"><a href="#12-脚注" class="headerlink" title="12.脚注"></a><strong>12.脚注</strong></h2><p>学习markdown[^markdown]<br>[^markdown]:一种网络笔记规范语言</p><h2 id="13-嵌入式代码"><a href="#13-嵌入式代码" class="headerlink" title="13.嵌入式代码"></a>13.嵌入式代码</h2><p>插入iframe即可</p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 语法 </tag>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/09/06/hello-world/"/>
      <url>/2023/09/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
