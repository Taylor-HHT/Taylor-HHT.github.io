<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Welcome Taylor</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>I want auroras and sad prose.</description>
    <pubDate>Mon, 03 Mar 2025 14:26:18 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>介绍MSA中经典数据集</title>
      <link>http://example.com/2025/03/03/%E4%BB%8B%E7%BB%8DMSA%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%95%B0%E6%8D%AE%E9%9B%86/</link>
      <guid>http://example.com/2025/03/03/%E4%BB%8B%E7%BB%8DMSA%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%95%B0%E6%8D%AE%E9%9B%86/</guid>
      <pubDate>Mon, 03 Mar 2025 14:24:57 GMT</pubDate>
      
      <description>&lt;p&gt;介绍MSA中经典数据集，分两模态（图文）和三模态（视频）数据集。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>介绍MSA中经典数据集，分两模态（图文）和三模态（视频）数据集。</p><span id="more"></span><h2 id="数据集样例简介"><a href="#数据集样例简介" class="headerlink" title="数据集样例简介"></a>数据集样例简介</h2><p>主要了解数据图片文本以及任务标签，对应的样例之间的关系，比如说一条文本对应多个图片之类的</p><h3 id="图文"><a href="#图文" class="headerlink" title="图文"></a>图文</h3><h4 id="1-YELP"><a href="#1-YELP" class="headerlink" title="1. YELP"></a>1. YELP</h4><ul><li>数据内容：来自Yelp.com评论网站，收集的是波士顿，芝加哥，洛杉矶，纽约，旧金山五个城市关于餐厅和食品的Yelp上的评论。</li><li>数据集规模：一共有44305条评论，244569张图片（每条评论的图片有多张），平均每条评论有13个句子，230个单词。</li><li>数据集的情感标注：是对每条评论的情感倾向打1,2,3,4,5五个分值。</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;Rating&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;BusinessId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;billys-sub-shop-boston&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;Text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;skip this house of slinging hash it has nothing on the ball the food is thrown on your plate broken yolks burnt home fries etc .|||dont go .&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;UserId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4-qeqjN2lgo0yzjYr-Of5Q&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;City&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Boston&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;Photos&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;Caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Photo of Billy&#x27;s Sub Shop - Boston, MA, United States&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ltEGQJMd4Pu79-tpO-00vQ&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;Caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Two eggs over easy with bacon and home fries&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-d-UehCQJkce1ukqMDXs5Q&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;Caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Eggs home fries and ham&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;3XqXPXfLF5bKr0J5qEVzLg&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;XXoVesfwJ-s4pD0EHoLGHA&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>JSON</th><th>PHOTO</th></tr></thead><tbody><tr><td>4.04GB compressed, 8.65GB uncompressed</td><td>6.93GB compressed, 7.11GB uncompressed</td></tr><tr><td>1 .tgz file compressed <br/>1 .pdf file and 5 .json files uncompressed</td><td>1 .tar file compressed<br/>1 .json file, 1 text file, 1 .pdf and 1 folder containing 200,100 photos</td></tr></tbody></table><ul><li><p>yelp_academic_dataset_business.json</p><ul><li>Contains business data including location data, attributes, and categories.</li></ul></li><li><p>yelp_academic_dataset_review.json</p><ul><li>Contains full review text data including the user_id that wrote the review and the business_id the review is written for.</li></ul></li><li><p>yelp_academic_dataset_user.json</p><ul><li>User data including the user’s friend mapping and all the metadata associated with the user.</li></ul></li><li><p>yelp_academic_dataset_checkin.json</p><ul><li>Checkins on a business.</li></ul></li><li><p>yelp_academic_dataset_tip.json</p><ul><li>Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.</li></ul></li><li><p>yelp_academic_dataset_photo.json</p><ul><li>Contains photo data including the caption and classification (one of “food”, “drink”, “menu”, “inside” or “outside”).</li></ul></li></ul><p><a href="https://www.yelp.com/dataset/documentation/main">https://www.yelp.com/dataset/documentation/main</a></p><p><a href="https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset/data">Yelp Dataset (kaggle.com)</a></p><p><a href="https://github.com/PreferredAI/vista-net">https://github.com/PreferredAI/vista-net</a></p><h4 id="2-Tumblr"><a href="#2-Tumblr" class="headerlink" title="2. Tumblr"></a>2. Tumblr</h4><p>无开源</p><p>1We cannot redistribute our dataset due to licensing restrictions, but the code to replicate the dataset and the results is available on</p><p><img src="https://img-blog.csdnimg.cn/20210428173803544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NDg2NTAx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/2021042817412246.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NDg2NTAx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="3-MVSA"><a href="#3-MVSA" class="headerlink" title="3. MVSA"></a>3. MVSA</h4><p>:white_check_mark:Multiple</p><ul><li><p>labelResultAll.txt</p><ul><li><table><thead><tr><th>ID</th><th>text,image</th><th>text,image</th><th>text,image</th></tr></thead><tbody><tr><td>2499</td><td>positive,positive</td><td>neutral,neutral</td><td>positive,positive</td></tr></tbody></table></li></ul></li><li><p>&#x2F;data</p><ul><li><p>2499.jpg</p></li><li><p>2499.txt: a sentence</p></li></ul></li></ul><p>&#x3D;&#x3D;single和multiple的区别&#x3D;&#x3D;</p><p><img src="/../../myBlog/images/image-20231015223426255.png" alt="image-20231015223426255"></p><h4 id="4-MSA-IR"><a href="#4-MSA-IR" class="headerlink" title="4. MSA-IR"></a>4. MSA-IR</h4><p>:negative_squared_cross_mark:文本shape(4600, 50, 200)，文本被处理；图像shape(4600, 224, 224, 3)</p><p>每个模态都有标签，同时还有整体标签以及图文匹配度</p><p>For the protection of copyright, we cannot provide the origin tweets in MSA-IR. Instead, we provide the preprocessed data in the form of <strong>pickles</strong>. </p><p>(1) <em><strong>Txt_label</strong></em> represents the sentiment polarity conveyed by textual description. Each text is given a label from the set {negative (labeled as -1), neutral (0), positive (1)}.<br>(2) <em><strong>Img_label</strong></em> represents the sentiment polarity conveyed by visual content. Each image is given a label from the set {negative (-1), neutral (0), positive (1)}.<br>(3) <em><strong>Multi_label</strong></em> represents the sentiment polarity conveyed by the whole image-text post. Each multimodal tweet is given a label from the set {negative (-1), neutral (0), positive (1)}.<br>(4) <em><strong>Cor_label</strong></em> represents the information relevance between an image and its corresponding text. Each multimodal tweet is given a label from the set {relevant (labeled as y), irrelevant (n)}.</p><ul><li>Note that, because Web users may post tweets without the restriction on image-text correlation, text and image in a message are not necessarily assigned the same sentiment label.</li><li><em><strong>One-hot encoding</strong></em> {positive: [1 0 0], neutral: [0 1 0], negative: [0 0 1]; relevant: [1 0], irrelevant: [0 1]}</li></ul><h4 id="5-Twitter-15-17——Aspect-level？？"><a href="#5-Twitter-15-17——Aspect-level？？" class="headerlink" title="5. Twitter 15&#x2F;17——Aspect-level？？"></a>5. Twitter 15&#x2F;17——<strong>Aspect-level</strong>？？</h4><p>:white_check_mark:关联数据集</p><p>image: 0001.jpg,  0002.jpg…</p><p>Twitter1517_texts and labels.xlsx:</p><table><thead><tr><th>ID</th><th>txt</th><th>new_image_id</th><th>original_image_id</th><th>txt_label<br/>(-1&#x2F;1&#x2F;0)</th><th>img_label<br/>(-1&#x2F;1&#x2F;0)</th><th>multi_label<br/>(-1&#x2F;1&#x2F;0)</th><th>cor_label<br/>(y&#x2F;n)</th></tr></thead><tbody><tr><td>1</td><td>How Jake Paul is changing the influencer game :</td><td>0001.jpg</td><td>17_06_10389.jpg</td><td>0</td><td>1</td><td>1</td><td>n</td></tr></tbody></table><p>方面词数据集</p><p>Entity-sensitive attention and fusion network for entity-level  multimodal sentiment classification</p><p><img src="/../../myBlog/images/image-20231016155105925.png" alt="image-20231016155105925"></p><ul><li>数据内容：包含文本和文本对应图片的多模态数据集，数据集标注了目标实体及对其图文中表达的情感倾向。</li><li>数据集规模：Twitter-15(3179&#x2F;1122&#x2F;1037)条带图片推文，推文平均长16.7；Twitter-17（3562&#x2F;1176&#x2F;1234）条带图片推文(train, dev, test)，推文平均长16.2个词</li><li>数据集的情感标注：情感标注为三分类</li></ul><p><img src="/../../myBlog/images/image-20231016163539151.png" alt="image-20231016163539151"></p><p>PS. 训练过程挖掉单词填空——IJCAI2019_data</p><p>Adapting BERT for Target-Oriented Multimodal Sentiment Classification</p><p><a href="https://github.com/jefferyYu/TomBERT">jefferyYu&#x2F;TomBERT: Dataset and codes for our IJCAI 2019 paper “Adapting BERT for Target-Oriented Multimodal Sentiment Classification” (github.com)</a></p><p><img src="/../../myBlog/images/image-20231014233145527.png" alt="image-20231014233145527"></p><h4 id="6-Multi-ZOL数据集——Aspect-level"><a href="#6-Multi-ZOL数据集——Aspect-level" class="headerlink" title="6. Multi-ZOL数据集——Aspect-level"></a>6. Multi-ZOL数据集——<strong>Aspect-level</strong></h4><p>原论文：Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis</p><ul><li>数据内容：数据集收集整理了IT信息和商业门户网站ZOL.com上的关于手机的评论。</li><li>数据集规模：原始数据有12587条评论(7359条单模态评论，5288条多模态评论)，覆盖114个品牌和1318种手机。其中的5288多模态评论，构成了Multi-ZOL数据集。在这个数据集中，每条多模态数据包含一个文本内容、一个图像集，以及至少一个但不超过六个评价方面。</li><li>数据集的情感标注：这六个方面分别是性价比、性能配置、电池寿命、外观与感觉、拍摄效果、屏幕。总共得到28469（&#x3D;5288*([1-6])）个方面。对于每个方面，都有一个从1到10的情感得分。</li></ul><p><a href="https://aistudio.baidu.com/datasetdetail/127654">Multi-ZOL 多模态情感分析数据集_数据集-飞桨AI Studio星河社区 (baidu.com)</a></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"></span><br><span class="line"> <span class="attr">&quot;brand&quot;</span><span class="punctuation">:</span> <span class="string">&quot;21克手机&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;cellphone_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;21克F1&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;review_title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;目前最好用的老人机，没有之一&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;review_data&quot;</span><span class="punctuation">:</span> <span class="string">&quot;屏幕清晰，色彩鲜艳（这个价位来说），电池大，而且不厚，按键大而且柔和，回馈力度也刚好。最值得称道的是通话声音，真的很好，浑厚明亮，最大音量也不会出现破音，环境嘈杂也听的一清二楚，适合老人。收音机不需要插耳机就能直接搜台，终于不是鸡肋了，还能接蓝牙耳机听歌。来电铃声也挺大。拔了电池不会错时间，这个比诺基亚的功能机好多了。。1、没有双卡版本，这个后续可以发展。系统字体均比较大，但是拨号界面的字体却不算很大，这个可以改进。。适合拿来做智能机备机，毕竟超长的续航能省不少事，也是和老人专门拿来打电话，操作上还是比较人性化的。。&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;review_total_score&quot;</span><span class="punctuation">:</span> <span class="number">9.4</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;items_score&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line"> <span class="attr">&quot;性价比&quot;</span><span class="punctuation">:</span> <span class="number">10.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;性能配置&quot;</span><span class="punctuation">:</span> <span class="number">8.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;电池续航&quot;</span><span class="punctuation">:</span> <span class="number">10.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;外观手感&quot;</span><span class="punctuation">:</span> <span class="number">10.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;拍照效果&quot;</span><span class="punctuation">:</span> <span class="number">10.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;屏幕效果&quot;</span><span class="punctuation">:</span> <span class="number">8.0</span></span><br><span class="line"> <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;reply_num&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;zan_num&quot;</span><span class="punctuation">:</span> <span class="number">87.0</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;img_num&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;img_path&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line"> <span class="string">&quot;..\\img\\https___pro-fd.zol-img.com.cn_g5_M00_02_0F_ChMkJlYLs9aIeFs3AC0Ie1J2oNAAADLcQLR2TYALQiT191.jpg&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="string">&quot;..\\img\\https___pro-fd.zol-img.com.cn_g5_M00_02_0F_ChMkJ1YLs9-IFdcmACcidxhfC4wAADLcQOh7NcAJyKP046.jpg&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="string">&quot;..\\img\\https___pro-fd.zol-img.com.cn_g5_M00_02_0F_ChMkJ1YLs-mIZH5DACYEsqcIr-oAADLcgC3cRYAJgTK207.jpg&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="string">&quot;..\\img\\https___pro-fd.zol-img.com.cn_g5_M00_02_0F_ChMkJ1YLs--ICST6ACAewfbwEYcAADLcgGmVS0AIB7Z276.jpg&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="string">&quot;..\\img\\https___pro-fd.zol-img.com.cn_g5_M00_02_0F_ChMkJlYLs_aILUb0ACEadavtNXIAADLcgJH87gAIRqN266.jpg&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="string">&quot;..\\img\\https___pro-fd.zol-img.com.cn_g5_M00_02_0F_ChMkJ1YLs_-ILNrGACV7aNMa-rEAADLcgMWiYsAJXuA572.jpg&quot;</span></span><br><span class="line"> <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>Twitters反讽数据集——反讽识别</p><p>反讽识别任务的目的是判断一段文档是否含有反讽表达。</p><ul><li>Twitter 反讽数据集<ul><li>Twitters反讽数据集构建自Twitter平台，其从Twitter上收集包含图片和一些特定话题标签(例如#sarcasm，等等)的英语推文，将其作为正例，并收集带有图片但没有此类标签的英语推文，作为反例。</li><li>数据集规模：数据集分为训练集、开发集和测试集，分别是19816,2410,2409条带图片推文。</li><li>数据集的情感标注：标注为是讽刺&#x2F;不是讽刺二分类</li></ul></li></ul><p>data: <a href="https://github.com/headacheboy/data-of-multimodal-sarcasm-detection">https://github.com/headacheboy/data-of-multimodal-sarcasm-detection</a></p><p>Implementation: <a href="https://github.com/ZLJ2015106/pytorch-multimodal_sarcasm_detection">https://github.com/ZLJ2015106/pytorch-multimodal_sarcasm_detection</a></p><h4 id="7-TumEmo"><a href="#7-TumEmo" class="headerlink" title="7. TumEmo"></a>7. TumEmo</h4><p>Image-text Multimodal Emotion Classification via Multi-view Attentional Network(IEEE Transactions on Multimedia, 2020)</p><p><a href="https://github.com/YangXiaocui1215/MVAN">YangXiaocui1215&#x2F;MVAN (github.com)</a></p><p>id_and_label.txt：Each line is a label of an image-text post corresponding to the ID (0,1,2, etc.).</p><p>image-text posts：containing images and texts, with ID being the image and text filename (0,1,2, etc.).</p><ul><li>数据内容：从Tumblr网站爬取</li></ul><p><img src="/../../myBlog/images/image-20231015232919227.png" alt="image-20231015232919227"></p><ul><li>数据集规模：195265个图文对</li></ul><p><img src="/../../myBlog/images/image-20231015224908162.png" alt="image-20231015224908162"></p><ul><li>数据集情感标注：情绪七分类</li></ul><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">id label</span><br><span class="line"><span class="number">0</span>  Angry</span><br><span class="line"><span class="number">1</span>  Angry</span><br><span class="line"><span class="number">2</span>  Angry</span><br><span class="line"><span class="number">3</span>  Angry</span><br><span class="line"><span class="number">4</span>  Angry</span><br><span class="line"><span class="number">5</span>  Angry</span><br><span class="line"><span class="number">6</span>  Angry</span><br></pre></td></tr></table></figure><h3 id="图文音"><a href="#图文音" class="headerlink" title="图文音"></a>图文音</h3><h4 id="8-CMU-MOSI-CMU-MOSEI——评论视频"><a href="#8-CMU-MOSI-CMU-MOSEI——评论视频" class="headerlink" title="8. CMU-MOSI &amp; CMU-MOSEI——评论视频"></a>8. CMU-MOSI &amp; CMU-MOSEI——评论视频</h4><p> In the SDK each dataset has three sets of content: <code>highlevel</code>, <code>raw</code> and <code>labels</code>. <code>highlevel</code> contains the extracted features for each modality (e.g OpenFace facial landmarks, openSMILE acoustic features) while <code>raw</code> contains the raw transctripts, phonemes.</p><p>.csd结尾的文件，是SDK中的一种称之为计算序列(computational sequences)的数据结构。</p><p>high level数据为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mmsdk <span class="keyword">import</span> mmdatasdk <span class="keyword">as</span> md</span><br><span class="line"></span><br><span class="line">visual_field = <span class="string">&quot;CMU_MOSI_Visual_Facet_41.csd&quot;</span></span><br><span class="line">acoustic_field = <span class="string">&quot;CMU_MOSI_OpenSmile_EB10.csd&quot;</span></span><br><span class="line">text_field = <span class="string">&quot;CMU_MOSI_TimestampedWordVectors.csd&quot;</span></span><br><span class="line">features = [</span><br><span class="line">  text_field, </span><br><span class="line">  visual_field, </span><br><span class="line">  acoustic_field</span><br><span class="line">]</span><br><span class="line">recipe = &#123;feat: os.path.join(<span class="string">&#x27;F:/files/JupyterProjects/data&#x27;</span>, feat).replace(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> features&#125;</span><br><span class="line"></span><br><span class="line">dataset = md.mmdataset(recipe)</span><br></pre></td></tr></table></figure><p>每一个dataset由三个模态的计算序列组成，在每一个计算序列里面，包含了多个视频，且每一个计算序列包含的id个数一致。如下图所示:</p><p><img src="F:\images\20210307113800406.png"></p><p>这个<strong>dataset</strong>的key是上面三个modality_file_name。而在每个模态里面，也是一个字典，key是视频的id，而value是一个元组——(feature,intervals)，前者是特征，后者是表示每一个时间戳开始和结束的时间。</p><p>Annotations are also computational sequences, since they are also just some values distributed on different time spans (e.g 1-3s is ‘angry’, 12-26s is ‘neutral’). Hence, we just add the label computational sequence to the dataset and then align to the labels. </p><p><a href="https://github.com/Justin1904/CMU-MultimodalSDK-Tutorials/blob/master/tutorial_interactive.ipynb">CMU-MultimodalSDK-Tutorials&#x2F;tutorial_interactive.ipynb at master · Justin1904&#x2F;CMU-MultimodalSDK-Tutorials (github.com)</a></p><p><a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">CMU-MultiComp-Lab&#x2F;CMU-MultimodalSDK (github.com)</a></p><ul><li><p>数据内容：MOSI数据集收集了YouTube上关于电影评论视频为主的视频博客(vlog)。</p></li><li><p>数据集规模：视频的长度从2-5分钟不等，总共随机收集了93个视频，这些视频来自89位不同的讲述者，其中有41位女性和48位男性，大多数演讲者的年龄大约在20到30岁之间，来自不同的种族背景。</p></li><li><p>数据集情感标注：这些视频的标注由来自亚马逊众包平台的五个标注者进行标注并取平均值，标注为从-3到+3的七类情感倾向。该数据集的情感标注不是观看者的感受，而是标注视频中的评论者的情感倾向。</p></li><li><p>数据内容：CMU-MOSEI收集的数据来自YouTube的独白视频，并且去掉了那些包含过多人物的视频。</p></li><li><p>数据集规模：最终的数据集包含3228个视频，23453个句子，1000个讲述者，250个话题，总时长达到65小时。</p></li><li><p>数据集情感标注：数据集既有情感标注又有情绪标注。情感标注是对每句话的7分类的情感标注【-3，3】。情绪标注是包含高兴，悲伤，生气，恐惧，厌恶，惊讶六个方面的情绪标注。</p></li></ul><p><del>1）YouTube数据集</del></p><p><del>2）ICT-MMMO数据集</del></p><ul><li><strong>面向对话视频的情感分类任务：</strong></li></ul><p>1）MELD数据集</p><p><a href="https://paperswithcode.com/paper/meld-a-multimodal-multi-party-dataset-for">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</a></p><ul><li>数据内容：MELD数据集源于EmotionLines数据集，后者是一个纯文本的对话数据集，来自于经典的电视剧老友记。MELD数据集是在此基础上的包含视频，文本，音频的多模态数据集。</li><li>数据集规模：最终的数据集包含13709个片段。</li><li>数据集情感标注：对个片段不仅有包含恐惧等七种在内的情绪标注，也有积极，消极，中性三分类的情感标注。</li></ul><p><a href="https://paperswithcode.com/sota/emotion-recognition-in-conversation-on-meld">https://paperswithcode.com/sota/emotion-recognition-in-conversation-on-meld</a></p><p>2）IEMOCAP数据集</p><ul><li><p>数据内容：IEMOCAP数据集是比较特殊的，它既不是收集自现有的YouTube等影视平台的用户上传视频，也不是收集自老友记等知名电视节目的，它是由10个演员围绕具体的主题进行表演并记录得到的多模态数据集。数据集收集的是由5个专业男演员和5个专业女演员，围绕主题进行会话表演得到的视频</p></li><li><p>数据集规模：总共包括4787条即兴会话和5255条脚本化会话，每个会话平均持续时间4.5秒，总时长11小时。</p></li><li><p>数据集情感标注：最终的数据标注是情绪标注，共有包含恐惧，悲伤在内的十个类别。</p></li><li><p><strong>面向新闻视频的情感分类任务：</strong></p></li></ul><p>1）News Rover Sentiment数据集</p><ul><li><p>数据内容：News  Rover  Sentiment数据集是新闻领域的数据集。该数据集使用的视频是在2013年8月13日至2013年12月25日之间录制的美国各种新闻节目和频道的视频。数据集按人员、职业进行了分类，视频长度限制在4到15秒之间。因为作者认为，很难在很短的视频中解读出人们的情绪，而15秒以上的视频可能会有多个带有不同情绪的语句。</p></li><li><p>数据集规模：最终整个数据集有929个片段。</p></li><li><p>数据集情感标注：对每一个片段都进行了三分类的情感标注。</p></li><li><p><strong>面向对话视频的反讽识别任务：</strong></p></li></ul><p><del>1）MUStARD数据集</del></p><ul><li>数据内容：MUStARD是一个关于多模态讽刺检测的数据集，它的来源很广泛，包含知名的电视剧，生活大爆炸，老友记，黄金女郎等，作者从这些包含讽刺的电视剧中收集了讽刺相关的视频，又从MELD数据集中获得非讽刺的视频。</li><li>数据集规模：最终的的数据集包含690个视频片段，其中345个是具有讽刺的视频片段，另外345个是不具有讽刺的视频片段</li><li>数据集情感标注：数据集的标注就是是否具有讽刺。</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</category>
      
      
      <comments>http://example.com/2025/03/03/%E4%BB%8B%E7%BB%8DMSA%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%95%B0%E6%8D%AE%E9%9B%86/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>调研MSA数据集中的baselines</title>
      <link>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84baselines/</link>
      <guid>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84baselines/</guid>
      <pubDate>Mon, 03 Mar 2025 14:14:00 GMT</pubDate>
      
      <description>&lt;p&gt;调研MSA中经典数据集的baselines，分三模态（视频）和两模态（图文）数据集。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>调研MSA中经典数据集的baselines，分三模态（视频）和两模态（图文）数据集。</p><span id="more"></span><h2 id="视频三模态数据集"><a href="#视频三模态数据集" class="headerlink" title="视频三模态数据集"></a>视频三模态数据集</h2><h3 id="MOSEI"><a href="#MOSEI" class="headerlink" title="MOSEI"></a>MOSEI</h3><ol><li>UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition</li><li>Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment</li><li>Multi-Modality Multi-Loss Fusion Network</li><li>A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis</li><li>Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation</li></ol><table><thead><tr><th>Model</th><th>Accuracy</th><th>MAE</th><th>F1</th><th>Ref Type</th><th>Code</th></tr></thead><tbody><tr><td><a href="https://paperswithcode.com/paper/unimse-towards-unified-multimodal-sentiment">UniMSE</a></td><td>87.50</td><td>0.523</td><td>87.46</td><td>2022</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/speech-text-dialog-pre-training-for-spoken">SPECTRA</a></td><td>87.34</td><td></td><td></td><td>2023</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/multi-modality-multi-loss-fusion-network">MMML</a></td><td>86.73</td><td>0.517</td><td>86.49</td><td>2023</td><td></td></tr><tr><td><a href="https://paperswithcode.com/paper/a-transformer-based-joint-encoding-for-1">Transformer-based joint-encoding</a></td><td>82.48</td><td></td><td></td><td>2020</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/mmlatch-bottom-up-top-down-fusion-for"> MMLatch</a></td><td>82.4</td><td>0.7</td><td></td><td>2021</td><td>√</td></tr></tbody></table><h3 id="MOSI"><a href="#MOSI" class="headerlink" title="MOSI"></a>MOSI</h3><ol><li>Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment</li><li>UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition</li><li>Cross-Modal BERT for Text-Audio Sentiment Analysis</li><li>Gated Mechanism for Attention Based Multimodal Sentiment Analysis</li><li>Multimodal Transformer for Unaligned Multimodal Language Sequences</li></ol><table><thead><tr><th>Model</th><th>Accuracy</th><th>MAE</th><th>F1</th><th>Ref Type</th><th>Code</th></tr></thead><tbody><tr><td><a href="https://paperswithcode.com/paper/speech-text-dialog-pre-training-for-spoken">SPECTRA</a></td><td>87.50</td><td></td><td></td><td>2023</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/unimse-towards-unified-multimodal-sentiment">UniMSE</a></td><td>86.9</td><td></td><td>86.42</td><td>2022</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/cross-modal-bert-for-text-audio-sentiment">CM-BERT</a></td><td>84.5%</td><td></td><td>84.5%</td><td>2020</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/gated-mechanism-for-attention-based"> Proposed: B2 + B4 w&#x2F; multimodal fusion</a></td><td>83.91%</td><td></td><td>81.17</td><td>2020</td><td></td></tr><tr><td><a href="https://paperswithcode.com/paper/190600295">MulT</a></td><td>83%</td><td></td><td>82.8</td><td>2019</td><td>√</td></tr></tbody></table><h3 id="MELD"><a href="#MELD" class="headerlink" title="MELD"></a>MELD</h3><p>Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also <strong>encompasses audio and visual modality</strong> along with text. <strong>MELD has more than 1400 dialogues and 13000 utterances from Friends TV series</strong>. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been <strong>labeled by any of these seven emotions – Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral)</strong> annotation for each utterance.</p><p><a href="https://affective-meld.github.io/">https://affective-meld.github.io/</a></p><ol><li><p>InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework</p></li><li><p>Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation</p></li><li><p>Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention</p></li><li><p>A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations</p></li><li><p>M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation</p></li><li><p>CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition</p></li></ol><table><thead><tr><th>Model</th><th>F1</th><th>Accuracy</th><th>Ref Type</th><th>Code</th></tr></thead><tbody><tr><td><a href="https://paperswithcode.com/paper/instructerc-reforming-emotion-recognition-in">InstructERC</a></td><td>69.15</td><td></td><td><em>arXiv</em>2023</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/supervised-prototypical-contrastive-learning">SPCL-CL-ERC</a></td><td>67.25</td><td></td><td>acl2022</td><td>√</td></tr><tr><td>DF-ERC</td><td>67.03</td><td></td><td><em>arXiv</em>2023</td><td></td></tr><tr><td><a href="https://paperswithcode.com/paper/hierarchical-dialogue-understanding-with-1">HiDialog</a></td><td>66.96</td><td></td><td><em>arXiv</em>2023</td><td>√</td></tr><tr><td>DualGATs</td><td>66.9</td><td></td><td>ACL2023</td><td></td></tr><tr><td>MultiEMO</td><td>66.74</td><td></td><td>ACL2023</td><td></td></tr><tr><td><a href="https://paperswithcode.com/paper/a-facial-expression-aware-multimodal-multi">FacialMMT</a></td><td>66.73</td><td></td><td>acl2023</td><td>√</td></tr><tr><td><a href="https://paperswithcode.com/paper/m2fnet-multi-modal-fusion-network-for-emotion">M2FNet</a></td><td>66.71</td><td>67.85</td><td>CVPR2022</td><td></td></tr><tr><td><a href="https://paperswithcode.com/paper/cfn-esa-a-cross-modal-fusion-network-with">CFN-ESA</a></td><td>66.70</td><td>67.85</td><td><em>arXiv</em>2023</td><td></td></tr><tr><td>UniMSE</td><td>65.51</td><td></td><td>acl2022</td><td></td></tr><tr><td>SACL-LSTM</td><td>58.44</td><td></td><td>ACL2023</td><td></td></tr></tbody></table><p>MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations ACL2023</p><p>66.74</p><p><img src="/../myBlog/images/image-20231030181210261.png" alt="image-20231030181210261"></p><p>DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations acl2023</p><p>66.9</p><p><img src="/../myBlog/images/image-20231030182920474.png" alt="image-20231030182920474"></p><p>Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations acl2023</p><p>58.44</p><p><img src="/../myBlog/images/image-20231030183144555.png" alt="image-20231030183144555"></p><p>Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition(<em>arXiv</em>2023)</p><p>67.03</p><p><img src="/../myBlog/images/image-20231030183223356.png" alt="image-20231030183223356"></p><h2 id="图文两模态数据集"><a href="#图文两模态数据集" class="headerlink" title="图文两模态数据集"></a>图文两模态数据集</h2><h3 id="yelp"><a href="#yelp" class="headerlink" title="yelp"></a>yelp</h3><p>源数据集论文：VistaNet: Visual aspect attention network for multimodal sentiment analysis</p><table><thead><tr><th>Model</th><th>Accuracy</th><th>Ref Type</th><th>Code</th></tr></thead><tbody><tr><td>UsbVisdaNet</td><td>62.83</td><td>mdpi2023</td><td></td></tr><tr><td>VisdaNet</td><td>62.32</td><td>mdpi2023</td><td></td></tr><tr><td>SFNN</td><td>60.8</td><td><em>CACRE</em>2020</td><td></td></tr><tr><td>GAFN</td><td>60.1</td><td><em>Knowledge-Based Systems</em>2022</td><td></td></tr><tr><td>VistaNet*</td><td>59.91</td><td>AAAI2019</td><td></td></tr><tr><td>HAN</td><td>57.33</td><td>NAACL2016</td><td></td></tr></tbody></table><p>UsbVisdaNet: User Behavior Visual Distillation and Attention Network for Multimodal Sentiment Classification</p><p><img src="/../myBlog/images/image-20231012182908372.png" alt="image-20231012182908372"></p><p>VisdaNet: Visual Distillation and Attention Network for Multimodal Sentiment Classification</p><p><img src="/../myBlog/images/image-20231011232040761.png" alt="image-20231011232040761"></p><p>SFNN: Semantic Features Fusion Neural Network for Multimodal Sentiment Analysis</p><p><img src="/../myBlog/images/image-20231012184838921.png" alt="image-20231012184838921"></p><p>Gated attention fusion network for multimodal sentiment classification</p><p><img src="/../myBlog/images/image-20231011230448945.png" alt="image-20231011230448945"></p><p> <a href="https://dl.acm.org/doi/abs/10.1145/3539597.3570437">Concept-oriented transformers for visual sentiment analysis</a> ACM WSDM2023</p><p>70.3</p><p><img src="/../myBlog/images/image-20231030143908609.png" alt="image-20231030143908609"></p><p> <a href="https://ieeexplore.ieee.org/abstract/document/9718029/">Learning disentangled representation for multimodal cross-domain sentiment analysis</a> IEEE TNNLS2022</p><p>69.1</p><p><img src="/../myBlog/images/image-20231030150022824.png" alt="image-20231030150022824"></p><p> <a href="https://ieeexplore.ieee.org/abstract/document/10149413/">Exploring Semantic Relations for Social Media Sentiment Analysis</a> IEEE&#x2F;ACM TASLP2023</p><p>65.03</p><p><img src="/../myBlog/images/image-20231030144213868.png" alt="image-20231030144213868"></p><p> <a href="https://ieeexplore.ieee.org/abstract/document/9859834/">HGLNET: A Generic Hierarchical Global-Local Feature Fusion Network for Multi-Modal Classification</a> ICME2022</p><p>63.92</p><p><img src="/../myBlog/images/image-20231030151610923.png" alt="image-20231030151610923"></p><p> <a href="https://ieeexplore.ieee.org/abstract/document/9121752/">LD-MAN: Layout-driven multimodal attention network for online news sentiment recognition</a> <em>IEEE Transactions on Multimedia</em>2020</p><p>61.22</p><p><img src="/../myBlog/images/image-20231030152115001.png" alt="image-20231030152115001"></p><p> <a href="https://dl.acm.org/doi/abs/10.1145/3583076">Hybrid Representation and Decision Fusion towards Visual-textual Sentiment</a><em>ACM Transactions on Intelligent Systems and Technology</em>2023</p><p>找不到&#x3D;&#x3D;</p><h3 id="MVSA"><a href="#MVSA" class="headerlink" title="MVSA"></a>MVSA</h3><p>&#x3D;&#x3D;MVSA-multiple&#x3D;&#x3D;</p><p>&#x3D;&#x3D;MVSA-single&#x3D;&#x3D;</p><p>ACL2023 Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection</p><p>源数据集论文：Sentiment analysis on multi-view social data</p><table><thead><tr><th>Model</th><th>single-a</th><th>single-f1</th><th>mul-a</th><th>mul-f1</th><th>ref type</th><th>code</th></tr></thead><tbody><tr><td>DVW Bert</td><td></td><td></td><td>93.66</td><td>93.48</td><td></td><td></td></tr><tr><td>MAMN</td><td>76.57</td><td>76.08</td><td>78.34</td><td>77.92</td><td></td><td></td></tr><tr><td>ITIN</td><td>75.19</td><td>74.97</td><td>73.52</td><td>73.49</td><td></td><td></td></tr><tr><td>MGNNS</td><td>73.77</td><td>72.7</td><td>72.49</td><td>69.34</td><td></td><td></td></tr><tr><td>MVAN-M</td><td>72.98</td><td>72.98</td><td>72.36</td><td>72.3</td><td></td><td></td></tr><tr><td>CLMLF</td><td>75.33</td><td>73.46</td><td>72</td><td>69.83</td><td></td><td></td></tr><tr><td>CMCN</td><td>73.61</td><td>75.03</td><td>70.45</td><td>74.77</td><td></td><td></td></tr><tr><td>Co-MN-Hop6</td><td>70.51</td><td>70.01</td><td>68.92</td><td>68.83</td><td></td><td></td></tr><tr><td>MultiSentiNet</td><td>69.84</td><td>69.63</td><td>68.86</td><td>68.11</td><td></td><td></td></tr></tbody></table><p>ACL2023 Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection</p><p>72.07</p><p><img src="/../myBlog/images/image-20231030165549886.png" alt="image-20231030165549886"></p><p>Multi-Level Attention Map Network for Multimodal Sentiment Analysis</p><p><img src="/../myBlog/images/image-20231014105211675.png" alt="image-20231014105211675"></p><p>Multimodal Sentiment Analysis With Image-Text Interaction Network</p><p><a href="https://orca.cardiff.ac.uk/id/eprint/148383/1/ITIN-Final.pdf">https://orca.cardiff.ac.uk/id/eprint/148383/1/ITIN-Final.pdf</a></p><p><img src="/../myBlog/images/image-20231011220840269.png" alt="image-20231011220840269"></p><p>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks</p><p><img src="/../myBlog/images/image-20231013194821967.png" alt="image-20231013194821967"></p><p>Image-text Multimodal Emotion Classification via Multi-view Attentional Network</p><p><img src="/../myBlog/images/image-20231013164546125.png" alt="image-20231013164546125"></p><p>CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection</p><p><img src="/../myBlog/images/image-20231013201150207.png" alt="image-20231013201150207"></p><p>Cross-Modal Complementary Network with Hierarchical Fusion for Multimodal Sentiment Classification</p><p><img src="/../myBlog/images/image-20231013201308817.png" alt="image-20231013201308817"></p><p>A Co-Memory Network for Multimodal Sentiment Analysis </p><p><img src="/../myBlog/images/image-20231013200021586.png" alt="image-20231013200021586"></p><p>MultiSentiNet: A Deep Semantic Network for Multimodal Sentiment Analysis</p><p><img src="/../myBlog/images/image-20231013200116329.png" alt="image-20231013200116329"></p><p> <a href="https://ieeexplore.ieee.org/abstract/document/9718029/">Learning disentangled representation for multimodal cross-domain sentiment analysis</a> IEEE TNNLS2022</p><p><img src="/../myBlog/images/image-20231030150022824.png" alt="image-20231030150022824"></p><p>MVSA-mul</p><p>Sentiment-aware multimodal pre-training for multimodal sentiment analysis</p><p><img src="/../myBlog/images/image-20231013195149777.png" alt="image-20231013195149777"></p><p>MVSA-SINGLE</p><p>Multi-Model Fusion Framework Using Deep Learning for Visual-Textual Sentiment Classification</p><p><img src="/../myBlog/images/image-20231014100347947.png" alt="image-20231014100347947"></p><p>Exploring Multimodal Sentiment Analysis via CBAM Attention and Double-layer BiLSTM Architecture</p><p><img src="/../myBlog/images/image-20231014100839586.png" alt="image-20231014100839586"></p><p>Feature-guided Multimodal Sentiment Analysis towards Industry 4.0</p><p><img src="/../myBlog/images/image-20231012190824601.png" alt="image-20231012190824601"></p><h3 id="TumEmo"><a href="#TumEmo" class="headerlink" title="TumEmo"></a>TumEmo</h3><table><thead><tr><th>Model</th><th>accuracy</th><th>f1</th><th>ref type</th><th>code</th></tr></thead><tbody><tr><td>MLP-FEF</td><td>77.74</td><td>77.77</td><td><em>arXiv</em>2022</td><td></td></tr><tr><td>MULSER</td><td>77.58</td><td>77.55</td><td><em>IEEE Transactions on Multimedia</em>2022</td><td></td></tr><tr><td>BIT</td><td>71.62</td><td>71.68</td><td><em>IJCNN</em>2023</td><td></td></tr><tr><td>MCFIT</td><td>70.85</td><td>70.79</td><td><em>Knowledge-Based Systems</em>2023</td><td></td></tr><tr><td>MGNNS</td><td>66.72</td><td>66.69</td><td>ACL-IJCNLP2021</td><td></td></tr><tr><td>MVAN-M*</td><td>66.46</td><td>63.39</td><td><em>IEEE Transactions on Multimedia</em>2020</td><td></td></tr></tbody></table><p>Improving Visual-textual Sentiment Analysis by Fusing Expert Features （<em>arXiv</em>2022）</p><p><img src="/../myBlog/images/image-20231016110838142.png" alt="image-20231016110838142"></p><p>Multimodal Emotion Classification with Multi-level Semantic Reasoning Network （<em>IEEE Transactions on Multimedia</em>2022）</p><p><img src="/../myBlog/images/image-20231016111433167.png" alt="image-20231016111433167"></p><p>BIT: Improving Image-text Sentiment Analysis via Learning Bidirectional Image-text Interaction （<em>IJCNN</em>2023）</p><p><img src="/../myBlog/images/image-20231016112352137.png" alt="image-20231016112352137"></p><p>Collaborative fine-grained interaction learning for image–text sentiment analysis （<em>Knowledge-Based Systems</em>2023）</p><p><img src="/../myBlog/images/image-20231016112101644.png" alt="image-20231016112101644"></p><p>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks（ACL-IJCNLP2021）</p><p><img src="/../myBlog/images/image-20231013194821967.png" alt="image-20231013194821967"></p><p>Image-text Multimodal Emotion Classification via Multi-view Attentional Network （<em>IEEE Transactions on Multimedia</em>）</p><p><img src="/../myBlog/images/image-20231013164546125.png" alt="image-20231013164546125"></p><p><del>Few-Shot Multi-Modal Sentiment Analysis with Prompt-Based Vision-Aware Language Modeling</del></p><p> <a href="https://arxiv.org/abs/2211.06607">Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts</a> arxiv2022</p><p>58.06</p><p><img src="/../myBlog/images/image-20231030160710263.png" alt="image-20231030160710263"></p><p> <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548306">Unified multi-modal pre-training for few-shot sentiment analysis with prompt-based learning</a> ACM MM2022</p><p>找不到&#x3D;&#x3D;</p><h3 id="Twitter-15-17"><a href="#Twitter-15-17" class="headerlink" title="Twitter 15&#x2F;17"></a>Twitter 15&#x2F;17</h3><p>&#x3D;&#x3D;侧重挖掘模态关联信息&#x3D;&#x3D;</p><p>Joint multimodal sentiment analysis based on information relevance (Information Processing and Management 2023)</p><p><img src="/../myBlog/images/image-20231012185451218.png" alt="image-20231012185451218"></p><p>&#x3D;&#x3D;Target-Oriented Multimodal Sentiment Classification&#x3D;&#x3D;</p><table><thead><tr><th>Model</th><th>t15-a</th><th>t15-f1</th><th>t17-a</th><th>t17-f1</th><th>ref type</th></tr></thead><tbody><tr><td>EF-CapTrBert-DE</td><td>77.92</td><td>73.9</td><td>72.3</td><td>70.2</td><td></td></tr><tr><td>CoolNet</td><td>79.92</td><td>75.28</td><td>71.64</td><td>69.58</td><td></td></tr><tr><td>TomBert</td><td>76.18</td><td>71.27</td><td>70.5</td><td>68.04</td><td></td></tr><tr><td>ESTR</td><td>71.36</td><td>64.28</td><td>65.8</td><td>62</td><td></td></tr></tbody></table><blockquote><p>Adapting BERT for Target-Oriented Multimodal Sentiment Classification</p><p><img src="/../myBlog/images/image-20231016163709184.png" alt="image-20231016163709184"></p><p>Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis</p><p>但这篇是基于方面的</p><p><img src="/../myBlog/images/image-20231016163057378.png" alt="image-20231016163057378"></p><p><img src="/../myBlog/images/image-20231016163118305.png" alt="image-20231016163118305"></p><p>Cross-modal fine-grained alignment and fusion network for multimodal aspect-based &gt; sentiment analysis</p><p><img src="/../myBlog/images/image-20231013205006912.png" alt="image-20231013205006912"></p><p>Entity-sensitive attention and fusion network for entity-level  multimodal sentiment &gt; &gt; classification（原论文）</p><p><img src="/../myBlog/images/image-20231016155308922.png" alt="image-20231016155308922"></p><p>Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation</p><p><img src="/../myBlog/images/image-20231016163605418.png" alt="image-20231016163605418"></p></blockquote><h3 id="Multi-ZOL"><a href="#Multi-ZOL" class="headerlink" title="Multi-ZOL"></a>Multi-ZOL</h3><table><thead><tr><th>Model</th><th>Accuracy</th><th>F1</th><th>Ref Type</th><th>Code</th></tr></thead><tbody><tr><td>MAMN</td><td>75.41</td><td>74.79</td><td></td><td></td></tr><tr><td>CMCN</td><td>74.28</td><td>71.51</td><td></td><td></td></tr><tr><td>CJMA</td><td>70.03</td><td>68.36</td><td></td><td></td></tr><tr><td>ModalNet</td><td>62.71</td><td>60.94</td><td></td><td></td></tr><tr><td>MIMN</td><td>61.59</td><td>60.51</td><td></td><td></td></tr></tbody></table><ol><li>Multi-Level Attention Map Network for Multimodal Sentiment Analysis</li></ol><p><img src="/../myBlog/images/image-20231014105627893.png" alt="image-20231014105627893"></p><ol start="2"><li>Cross-Modal Complementary Network with Hierarchical Fusion for Multimodal Sentiment Classification</li></ol><p><img src="/../myBlog/images/image-20231013201317958.png" alt="image-20231013201317958"></p><ol start="3"><li>A Conditioned Joint-Modality Attention Fusion Approach for Multimodal Aspect-Level Sentiment Analysis</li></ol><p><img src="/../myBlog/images/image-20231014110504142.png" alt="image-20231014110504142"></p><ol start="4"><li>ModalNet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network</li></ol><p><img src="/../myBlog/images/image-20231014170333642.png" alt="image-20231014170333642"></p><ol start="5"><li>Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis（他提出</li></ol><p><img src="/../myBlog/images/image-20231014104337227.png" alt="image-20231014104337227"></p><ol start="6"><li>Visual Enhancement Capsule Network for Aspect-Based Multimodal Sentiment Analysis</li></ol><p><img src="/../myBlog/images/image-20231013214621637.png" alt="image-20231013214621637"></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</category>
      
      
      <comments>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84baselines/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>调研MSA中的方法（四）：对比学习</title>
      <link>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</link>
      <guid>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</guid>
      <pubDate>Mon, 03 Mar 2025 13:58:12 GMT</pubDate>
      
      <description>&lt;p&gt;这一章介绍四篇MSA中经典的结合对比学习方法的文章，末尾附上这四章的小总结~本系列共四章：多视图、对抗学习、元学习、对比学习。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>这一章介绍四篇MSA中经典的结合对比学习方法的文章，末尾附上这四章的小总结~本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="4-对比学习"><a href="#4-对比学习" class="headerlink" title="4. 对比学习"></a>4. 对比学习</h4><h5 id="（1）ConFEDE-Contrastive-Feature-Decomposition-for-Multimodal-Sentiment-Analysis-ACL2023"><a href="#（1）ConFEDE-Contrastive-Feature-Decomposition-for-Multimodal-Sentiment-Analysis-ACL2023" class="headerlink" title="（1）ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis (ACL2023)"></a>（1）ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis (ACL2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;特征分解，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;差异性unique和一致性common 信息对于整体情感的含义非常有必要，尤其是对于sarcasm任务，</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;将每个模态分解为一个相似性特征（similarity feature）和一个相异性特征（dissimilarity feature），并以文本的相似性特征为锚，建立所有分解特征之间的对比关系。</p><p><img src="F:/files/myBlog/images/1ff561b2894ddb418ae717d2b428e1d9.png" alt="image-20240319221241287"></p><p>如上图所示，文章的贡献是在中间的部分，利用不同的投影器将每个模态的初始特征（T , V , A）分解为相似性特征（$T_s, V_s, A_s$）和相异性特征 （$T_d, V_d, A_d$）。（每个投影器都由 layer normalization，带有 Tanh 激活函数的linear layer 以及 dropout layer 构成），然后，通过对比特征分解学习不断地更新这六个分解特征，并将其融合到 ConFEDE 模型中，利用多任务学习目标损失函数进行训练。Loss损失计算对应图下面的三个任务，具体如下所示：$\mathcal{L}<em>{\text {all }}&#x3D;\mathcal{L}</em>{\text {pred }}+\beta_{\text {uni }} \mathcal{L}<em>{\text {uni }}+\beta</em>{\mathrm{cl}} \mathcal{L}<em>{\mathrm{cl}}$。其中，$\mathcal{L}</em>{\text {pred }}$ 为多模态预测损失， $\mathcal{L}<em>{\text {uni }}$ 为单模态预测损失，  $\mathcal{L}</em>{\text {cl}}$ 为对比损失。</p><p>① <strong>单模态预测损失：</strong>是将6 个分解的特征分别输入一个权重共享的 MLP 分类器，得到 6 个预测结果。<strong>相似性特征（$T_s, V_s, A_s$） 通过 MLP 映射来预测多模态标签  ，相异性特征（$T_d, V_d, A_d$）通过 MLP 映射来预测特定模态标签</strong> ，但是当特定模态标签没有的时候，例如MOSI与MOSEI数据集，相异性特征（$T_d, V_d, A_d$）也被用来预测多模态标签。这样做的原因是，<strong>让相似性特征（$T_s, V_s, A_s$）通过样本的整体多模态标签捕获不同模态之间共享的一致信息，让相异性特征（$T_d, V_d, A_d$）保留由单模态标签表示的特定模态信息。</strong></p><p>② <strong>对比损失：</strong>在一个简单的联合对比损失中进行对比：相似样本与相异样本进行对比（inter-sample）；同一样本相似特征与相异特征（intra-sample）进行对比。</p><p><img src="F:/files/myBlog/images/image-20241011155522619.png" alt="image-20241011155522619"></p><p>其中，(a,p), (a,k) 表示一对分解后的特征向量。$(a, p) \in \mathcal{P}^{i},(a, k) \in \mathcal{P}^{i} \text { or } \mathcal{N}^{i}$</p><p>③ <strong>数据采样：</strong>根据多模态特征和多模态标签<strong>检索给定样本的相似样本，从而在样本间执行有监督的对比学习</strong>。对于 D 中的每个样本对( i , j )，计算它们之间的余弦相似度得分（内积）。然后，检索每个样本的候选相似&#x2F;相异样本集。对于每个样本 i ，根据相似度得分从高到低将具有相同多模态标签$y_m^i$的样本排序，作为候选相似样本集 $S_0^i$。将标签$y_m^i$ 以外的样本排序，作为候选相异样本集 $S_1^i$。</p><p>从候选相似样本集$S_0^i$ 中随机选取两个余弦相似度得分较高的近似样本，与样本 i构成样本间正对，记为$\text{Neighbor}^i$；从候选相异样本集 $S_1^i$中随机选取四个不同的样本组成样本间负对，记为$\text{Outlier}^i$（其中两个样本具有较低的余弦相似度分数$$\text{Outlier}^i_1$$ ，另外两个样本具有较高的余弦相似度分数$$\text{Outlier}^i_2$$）。</p><p><strong>为什么这样选择？</strong>通常倾向于选择$\text{Neighbor}^i$与$$\text{Outlier}^i_1$$ 中的样本分别与样本 i 形成正对和负对。但是由于$\text{Outlier}^i_2$中的样本与样本 i  具有不同的标签，并且具有相似的语义信息，所以$\text{Outlier}^i_2$中的样本很难与样本 i 区分开。故，将$\text{Outlier}^i_2$中的样本也添加到$\text{Outlier}^i$中。通过对比学习让$\text{Outlier}^i_2$中的样本与样本 i 区分开。</p><p>④<strong>样本内intra-sample的正对&#x2F;负对构建：</strong>使用六个分解特征形成样本内的正&#x2F;负对。选择文本相似性特征 $T_s^i$ 作为锚点，使视觉和听觉相似性特征 $A_s^i$ 与 $V_s^i$ 向 $T_s^i$ 靠拢，同时将所有模态中的相异性特征 $T_d^i$ , $V_d^i$ , $A_d^i$ 远离  $T_s^i$ 。</p><p><img src="F:/files/myBlog/images/image-20241011161442040.png" alt="image-20241011161442040"></p><p>注意：$\text{Neighbor}^i$与$\text{Outlier}^i$ 分别代表样本 i 的相似样本和相异样本，$$<br>j \in \text { Neighbor }^i \cup \text { Outlier }^i<br>$$所起的作用是扩大对比范围。</p><p>⑤<strong>样本间inter-sample的正对&#x2F;负对构建：</strong>根据数据采样器采样得到的$\text{Neighbor}^i$与$\text{Outlier}^i$ 构建样本间的正对&#x2F;负对。</p><p><img src="F:/files/myBlog/images/image-20241011162146497.png" alt="image-20241011162146497"></p><p>注意：只使用相似性特征来获得样本间的配对，因为同一类别中相似性样本的相似性特征应该很接近，而不同类别中样本的相似性特征应该相距较远。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;在没有单模态标签的情况下，模型效果表现不好，因为在单模态预测任务中，是用多模态的标签监督学习，会起到干扰的作用</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;CMU-MOSI，CMU-MOSEI，CH-SIMS（带有单模态标签）</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;MOSI和MOSEI 结果不是很高，SIMS还行</p><p><img src="F:/files/myBlog/images/image-20241011162737378.png" alt="image-20241011162737378"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/XpastaX/ConFEDE">https://github.com/XpastaX/ConFEDE</a></p><h5 id="（2）Multi-level-Contrastive-Learning-Hierarchical-Alleviation-of-Heterogeneity-in-Multimodal-Sentiment-Analysis（IEEE-Transactions-on-Affective-Computing-2024）"><a href="#（2）Multi-level-Contrastive-Learning-Hierarchical-Alleviation-of-Heterogeneity-in-Multimodal-Sentiment-Analysis（IEEE-Transactions-on-Affective-Computing-2024）" class="headerlink" title="（2）Multi-level Contrastive Learning: Hierarchical Alleviation of Heterogeneity in Multimodal Sentiment Analysis（IEEE Transactions on Affective Computing 2024）"></a>（2）Multi-level Contrastive Learning: Hierarchical Alleviation of Heterogeneity in Multimodal Sentiment Analysis（IEEE Transactions on Affective Computing 2024）</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对比学习；多层级融合</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;现有方法比较少考虑不同的模态特征存在<strong>异构性</strong>的问题，这会影响后续的融合过程。模态的异构性是由模态之间<strong>不同的特征分布和不同的表征空间</strong>导致的。比如现有的方法通过注意力来融合，但不考虑异构性问题的话，数据分布之间的不一致会导致不平均的注意力权重分配。异构性也来自于不同模态组合，比如双模态和单模态之间的信息差异。<strong>而对比学习可以解决异构性问题，因为它可以把不同模态的数据投射到同一特征空间。</strong></p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;文章主要针对的点是异质性问题出现在模态融合的各个阶段：不同模态（文本、音频、视觉）的低层次特征表现（如纹理、语音音高，basic information）和高层次的语义特征（semantic information features）。所以在设计模型的时候<strong>把对比学习用在了三个阶段</strong>（渐进式融合，也是比较自然的想法），并且融合的过程也是分两步走。框架图如下，在first-level，second-level，third-level层都分别用到了对比学习。</p><p><img src="F:/files/myBlog/images/image-20240928165118509-1728702396078-1728702401197.png" alt="image-20240928165118509"></p><p>如框架图所示：</p><ul><li>第一层对比学习在多模态特征融合的早期阶段缓解了单模态之间的异质性。</li></ul><p><img src="F:/files/myBlog/images/image-20241012113714745.png" alt="image-20241012113714745"></p><ul><li>第二层对比学习减轻了单模态和融合模态之间的异质性。</li></ul><p><img src="F:/files/myBlog/images/image-20241012113921820.png" alt="image-20241012113921820"></p><ul><li>在第三层上，引入了张量卷积融合模块，该模块从融合的模态中提取高层语义特征，并通过对比学习在更高的特征层面缓解异构性。</li></ul><p><img src="F:/files/myBlog/images/image-20241012114140724.png" alt="image-20241012114140724"></p><p>对于融合过程，文章提出<strong>多层卷积融合（MCF）</strong>的方法模拟连续融合过程，多层对比学习和MCF可以协同操作，产生最优的融合结果。对于MCF模块，文章是受<strong>张量融合网络（TFN）</strong>的启发，TFN通过使用张量的外积将不同模态的数据融合在一起，形成一个高维的张量表示，这个张量表示捕获了不同模态间的相互作用。但由于这个张量表示含有冗余信息，因此作者改进引入多层的张量卷积融合。顾名思义，卷积融合就是用了卷积的操作来融合。不同于TFN的单次外积，MCF会先生成一个多模态“图像”（通过外积生成），然后使用二维卷积进行特征提取。这种方法可以像处理图像一样，多角度提取融合模态中的高级特征，因此达到逐步减少冗余信息的目的。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI,MOSEI,SIMS</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;挺高的</p><p><img src="F:/files/myBlog/images/image-20241012114225268.png" alt="image-20241012114225268"></p><p><img src="F:/files/myBlog/images/image-20241012114240460.png" alt="image-20241012114240460"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/Zhudogsi/MCL-MCF">https://github.com/Zhudogsi/MCL-MCF</a></p><h3 id="二、总结的问题（及对应的方法）"><a href="#二、总结的问题（及对应的方法）" class="headerlink" title="二、总结的问题（及对应的方法）"></a>二、总结的问题（及对应的方法）</h3><ol><li>不同模态表示信息不匹配&#x2F;矛盾：①学习模态的差异性unique和一致性common information，特征分解（multi-view）；②meta-learning去学习单模态标签。（讽刺识别任务，图像是开心的表情，可是文本是伤感的，当模态信息有冲突，如何挖掘出背后作者的情绪是积极&#x2F;消极的，这个对应就是 low-shared information的情况。）</li><li>异构性：减少分布差距。有mapping, translation, <strong>adversarial learning，对比学习</strong>方法。比如说对比学习方法，可以实现自监督，一是通过拉近同一样本不同模态（正样本）的距离，拉远不同样本不同模态（负样本）的距离，可以对齐不同模态的表示和拉大负样本间的差异性，从而学习到更加鲁棒的特征。二是可以使不同模态的数据投射到同一特征空间中，并在这个空间内保持模态间的紧密联系。这对于融合多模态特征、提高模态间的协同作用十分有帮助。</li><li>data imbalance问题：研究人员开始使用生成对抗学习来生成符合原始数据分布的新样本。具体来说，先前的工作通过最小化生成器和判别器学习到的数据分布来生成新的样本。（Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations–related work提到）</li><li>自适应权重：主要用到元学习，该策略主要是通过一小部分带有单模态标签的手工标注数据来学习每个模态的权重，并动态调整这些权重以适应不同的情感表达。也就是说，对于每个sample，模型可以根据模态之间的差异性和模态缺失情况，自动调整不同模态的权重。比如说，当某个模态信息对情感预测贡献较大时，模型会为该模态分配更高的权重。</li></ol><h3 id="三、其他Tips"><a href="#三、其他Tips" class="headerlink" title="三、其他Tips"></a>三、其他Tips</h3><ol><li>自适应</li><li>分层次</li><li>有监督和无监督</li><li>翻译网络：用来在图像和文本模态之间进行信息转换。比如说，通过翻译网络将图像模态的信息转换为文本模态的表示。这个翻译网络的目标是捕捉模态间共享的情感信息，特别是在一种模态缺失的情况下，能够利用另一种模态的翻译表示来替代缺失模态的数据。</li><li>学单模态标签：使用多模态标签来训练单模态信号可能会引入噪声，影响模型的性能。不同模态可能传达不同的情感方面，导致多模态样本中各模态之间的标签不一致，进一步加剧了噪声标签问题。</li><li>对抗学习分阶段，或者是突出文本的重要性，或者是自适应模态的权重</li></ol>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">对比学习</category>
      
      
      <comments>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>调研MSA中的方法（三）：元学习</title>
      <link>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%85%83%E5%AD%A6%E4%B9%A0/</link>
      <guid>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%85%83%E5%AD%A6%E4%B9%A0/</guid>
      <pubDate>Mon, 03 Mar 2025 13:58:00 GMT</pubDate>
      
      <description>&lt;p&gt;这一章介绍四篇MSA中经典的结合元学习方法的文章~本系列共四章：多视图、对抗学习、元学习、对比学习。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>这一章介绍四篇MSA中经典的结合元学习方法的文章~本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="3-meta-learning"><a href="#3-meta-learning" class="headerlink" title="3. meta-learning"></a>3. meta-learning</h4><h5 id="（1）Meta-Learn-Unimodal-Signals-with-Weak-Supervision-for-Multimodal-Sentiment-Analysis-（arXiv-2024）"><a href="#（1）Meta-Learn-Unimodal-Signals-with-Weak-Supervision-for-Multimodal-Sentiment-Analysis-（arXiv-2024）" class="headerlink" title="（1）Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis （arXiv 2024）"></a>（1）Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis （arXiv 2024）</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;元学习，单模态标签学习，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;noisy label problem: 单模态表征学习如果用多模态的标签训练，会影响单模态的学习</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;The first to introduce metalearning  into MSA for learning accurate unimodal labels</p><p>主要思想是通过弱监督学习单模态标签，以改进多模态情感分析。提出框架Meta Uni-label Generation, MUG。MUG框架包括三个主要阶段：<strong>单模态网络的构建、多模态框架的预训练和元学习策略的应用</strong>。</p><p>①预训练：设计了基于对比学习的投影模块（Contrastive-based Projection Module, CPM），以缩小单模态和多模态表示之间的差距。具体来说，将多模态表示投影到单模态嵌入空间，并使用投影的多模态嵌入训练单模态预测器。通过对比学习提高单模态和多模态表示之间的互信息，减少它们之间的分布差距。预训练阶段图如下：</p><p><img src="F:/files/myBlog/images/image-20241011211004154.png" alt="image-20241011211004154"></p><p><img src="F:/files/myBlog/images/image-20241011211253416.png" alt="image-20241011211253416"></p><p>②元单标签校正网络（Meta Uni-label Correction Network, MUCN）的元学习过程：包括单模态去噪任务和多模态去噪任务。</p><ul><li>在元训练阶段（unimodal denoising task），训练MUCN以去噪手动损坏的多模态标签，并恢复原始多模态标签。通过高斯噪声防止MUCN学习身份映射，并提供模型学习最优单模态标签的能力。</li><li>在元测试阶段（multimodal denoising task），利用干净的多模态标签和表示指导MUCN的学习。通过估计MUCN是否能恢复干净的多模态标签来评估其有效性。</li><li>采用双层优化策略以提高MUCN的训练效果：如果MUCN在单模态去噪任务的参数在多模态去噪任务上表现的好，那就用原的这个参数来更新；如果表现不好，那就得加上利用多模态去噪任务的损失来进行参数更新。</li><li>单模态去噪和多模态去噪两个任务的区别，本质都是要恢复加了噪声的多模态标签y：<ul><li>单模态去噪：输入是$x_m$和加了高斯的多模态真实标签y，想要输出尽可能靠近真实标签y</li><li>多模态去噪：输入是利用统一多模态表示x映射后的 $x_m\prime$ 和加了高斯的单模态预测标签 $\hat{y}_m\prime$ （利用了预训练阶段的Projection Layer和Unimodal Predictor），想要输出尽可能靠近真实标签 y。</li></ul></li></ul><p><img src="F:/files/myBlog/images/image-20241011211441814.png" alt="image-20241011211441814"></p><p>③多任务训练：联合训练单模态和多模态学习任务，以提取更具辨别力的单模态特征。具体是单模态任务训练使用元学习阶段生成的校正单模态标签进行单模态任务训练；多模态任务训练使用多模态标签指导多模态任务学习。</p><p>④总结：通过这种方式，MUG能够最大限度地利用现有的多模态信息，提取更具辨别力和表达力的单模态特征，从而提高多模态情感分析的性能。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;CMU-MOSI；CMU-MOSEI；SIMS（中文数据集）</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20240930112043696-1728651971309.png" alt="image-20240930112043696"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;无</p><h5 id="（2）Learning-to-Learn-Better-Unimodal-Representations-via-Adaptive-Multimodal-Meta-Learning-TAC2023"><a href="#（2）Learning-to-Learn-Better-Unimodal-Representations-via-Adaptive-Multimodal-Meta-Learning-TAC2023" class="headerlink" title="（2）Learning to Learn Better Unimodal Representations via Adaptive Multimodal Meta-Learning(TAC2023)"></a>（2）Learning to Learn Better Unimodal Representations via Adaptive Multimodal Meta-Learning(TAC2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;元学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;解决以往多模态融合只能获得次优的单模态表征；缩小各模态之间的分布差异</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012102129511.png" alt="image-20241012102129511"></p><p><strong>①Distribution Transformation Layer（DTL）：</strong>减小模态特征分布之间的间隙，促进多模态融合。我们首先将单模态表示的分布转换为高斯分布，然后将转换后的分布在不同模态之间进行匹配。单模态表示转为高斯分布过程采用参数化技巧，以避免梯度计算过程中的随机性：</p><p><img src="F:/files/myBlog/images/image-20241012101742141.png" alt="image-20241012101742141"></p><p>通过KL散度 Loss来缩小gap：</p><p><img src="F:/files/myBlog/images/image-20241012101809280.png" alt="image-20241012101809280"></p><p><strong>②Adaptive Multimodal Meta-Learning：</strong></p><p>a. 元训练阶段（“Inner-Update Stage”）：为每种模态分配特定的元学习优化程序，从而获得每种模态的最佳的单模态表征，并将其用于多模态融合。与之前读的meta learning学习权重不同的是，这里学习的是学习率，控制更新方向和速度（归根到底还是Loss的设置和训练的方式和阶段不一样）：</p><p><img src="F:/files/myBlog/images/image-20241012101926713.png" alt="image-20241012101926713"></p><p>单模态表征学习网络的参数在inner-update and outer-update stages被更新两次。</p><p>b.元测试阶段（“Outer-Update Stage”）：</p><p><img src="F:/files/myBlog/images/image-20241012102020083.png" alt="image-20241012102020083"></p><p>这里的元学习指多模态任务信息指导单模态网络的更新。与传统meta learning任务不同的是，传统meta的元训练和元测试阶段数据不同，但setting相同。而这篇文章是数据相同，setting不同。我们的目标是将在元训练(单模态)任务中优化的unimodal learners适应到元测试(多模态)任务中，其中两个任务的目标是不同的。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;CMU-MOSEI and CMUMOSI</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;不高</p><p><img src="F:/files/myBlog/images/image-20241012102625880.png" alt="image-20241012102625880"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;无</p><h5 id="（3）Crossmodal-Translation-based-Meta-Weight-Adaption-for-Robust-Image-Text-Sentiment-Analysis-ACM-MM2021"><a href="#（3）Crossmodal-Translation-based-Meta-Weight-Adaption-for-Robust-Image-Text-Sentiment-Analysis-ACM-MM2021" class="headerlink" title="（3）Crossmodal Translation based Meta Weight Adaption for Robust Image-Text Sentiment Analysis (ACM MM2021)"></a>（3）Crossmodal Translation based Meta Weight Adaption for Robust Image-Text Sentiment Analysis (ACM MM2021)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;翻译网络，元学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;模态独特性的重要性，missing modality</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p>①跨模态翻译网络：用来在图像和文本模态之间进行信息转换。比如说，通过翻译网络将图像模态的信息转换为文本模态的表示。这个翻译网络的目标是捕捉模态间共享的情感信息，特别是在一种模态缺失的情况下，能够利用另一种模态的翻译表示来替代缺失模态的数据。</p><p><img src="F:/files/myBlog/images/image-20241012103210831.png" alt="image-20241012103210831"></p><p>在原始数据集手动给一些样本标注单模态的标签，标记为$D_{meta}$，剩下的数据集是$D_{train}$。</p><p>②普通优化pseudo-updating of the backbone network $\theta$：**在第$\tau$步，使用$D_{train}$，首先前向计算backbone网络$\Phi_b(\cdot;\theta^{(\tau)})$，网络输入输出如下：</p><p><img src="F:/files/myBlog/images/image-20241012103541059.png" alt="image-20241012103541059"></p><p>然后还要计算unimodal weight generation network ，网络输出的是每个样本的单模态权重：</p><p><img src="F:/files/myBlog/images/image-20241012103600142.png" alt="image-20241012103600142"></p><p>这一阶段的训练目标$L_1$如下：</p><p><img src="F:/files/myBlog/images/image-20241012103620308.png" alt="image-20241012103620308"></p><p>这里的loss，第一部分$L_c$是交叉熵，第二部分Σ项是权重比例乘以交叉熵项，与第一项交叉熵比较真实标签y和模型预测样本标签 $\hat{y}$ 不同的是，这一项比较的是真实标签y和单模态标签$\hat{y}_u$，也就是说这里的单模态标签会尽可能往总的标签预测，用统一的多模态label来监督单模态情感分析网络的学习，这样做的问题是会导致独立的单模态信息的扰动。最后一项 $L_r$ 。最后一项$L_r$是crossmodal translation网络的Loss值。</p><p>用这个$L_1$来更新旧网络参数$\theta^{(\tau)}$，得到新网络参数$\hat{\theta}^{(\tau)}$：</p><p><img src="F:/files/myBlog/images/image-20241012103641734.png" alt="image-20241012103641734"></p><p>总的流程图如下：</p><p><img src="F:/files/myBlog/images/image-20241012103712230.png" alt="image-20241012103712230"></p><p>可以看到，前向计算涉及backbone和UWG网络，后向更新参数是作用在backbone。</p><p>③元学习指导refining the unimodal weight generation network $\omega$：这一阶段主要的目标是给UWG网络的更新提供指导。因为前面第一阶段是用统一的多模态label来监督单模态情感分析网络的学习<strong>（bad）</strong>，这里引用了元学习的指导，使用人工标注好的单模态标签数据集$D_{meta}$。</p><p>训练目标如下：<img src="F:/files/myBlog/images/image-20241012103803300.png" alt="image-20241012103803300"></p><p>这里第二项Σ就是用的真实单模态标签$y_u$（人工标注）和预测标签$\hat{y_u}$。<strong>（good）</strong>不仅如此，这个Loss与第一阶段更新相比，没有translation network的$L_r$。</p><p>参数更新UWG网络：<img src="F:/files/myBlog/images/image-20241012103910297.png" alt="image-20241012103910297"></p><p><img src="F:/files/myBlog/images/image-20241012103936424.png" alt="image-20241012103936424"></p><p>可以看到，前向计算涉及backbone，后向更新参数是作用在UWG网络。</p><p>④使用 更新的UWG网络 得到的 单模态权重 来更新backbone $\theta$：这一个阶段的训练目标如下：</p><p> <img src="F:/files/myBlog/images/image-20241012104013814.png" alt="image-20241012104013814"></p><p>与第二阶段相比，这里的Σ项多了第二个阶段更新后的权重。计算与第一个阶段的训练目标是一样的，这不过这一阶段经历了第二阶段的人工单模态标签的指导，$\hat{y}_u$会更准确一些。</p><p>参数更新backbone：<img src="F:/files/myBlog/images/image-20241012104028870.png" alt="image-20241012104028870"></p><p><img src="F:/files/myBlog/images/image-20241012104059546.png" alt="image-20241012104059546"></p><p>可以看到，前向计算涉及backbone和UWG网络，后向更新参数是作用在backbone。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;①数据集可以从图文两模态拓展到视频三模态等，该文章模型的设计是需要单个模态的标签的，后续拓展的数据集中的单模态标签可以是自动生成的，应用范围可以更加广泛。②文章是用了跨模态翻译的方法处理模态缺失问题，之前有泛泛地了解到对抗学习也被用于处理模态缺失，对抗学习较于前者是侧重于生成更具逼真性的模态补全表示。具体来说，生成器负责生成缺失模态，判别器负责判断生成模态的真实性，通过对抗学习提升模态补全的质量。模型可以生成缺失模态的补全数据，从而提升模型的性能。具体模型有生成对抗网络（GAN）。③元学习在这篇文章的作用有些类似于之前看到的用多任务学习来辅助多模态情感分析任务，也是借助外部的信息（另一个任务或者另一些样本的信息）去辅助本身的任务。但两者对比，多任务学习更多是学习不同任务间共享的有效信息（因此要求不同任务之间是得有关联的），而本文提出的元学习策略倾向于捕获不同模态之间独有的信息，并且元学习更加轻量化，需要的样本数更少，还可以快速适应新任务。后续会调研一下，尝试将对抗学习与元学习结合，来进一步提高模型的稳健性。</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;image text sentiment analysis</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MVSA-Single and MVSA-Multiple，TumEmo</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012104300919.png" alt="image-20241012104300919"></p><p><img src="F:/files/myBlog/images/image-20241012104308536.png" alt="image-20241012104308536"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D; <a href="https://github.com/thuiar/CTMWA">https://github.com/thuiar/CTMWA</a></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/">元学习</category>
      
      
      <comments>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%85%83%E5%AD%A6%E4%B9%A0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>调研MSA中的方法（二）：对抗学习</title>
      <link>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0/</link>
      <guid>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0/</guid>
      <pubDate>Mon, 03 Mar 2025 13:54:45 GMT</pubDate>
      
      <description>&lt;p&gt;这一章介绍四篇MSA中经典的结合对抗学习方法的文章~本系列共四章：多视图、对抗学习、元学习、对比学习。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>这一章介绍四篇MSA中经典的结合对抗学习方法的文章~本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="2-对抗学习"><a href="#2-对抗学习" class="headerlink" title="2. 对抗学习"></a>2. 对抗学习</h4><h5 id="（1）Supervised-Adversarial-Contrastive-Learning-for-Emotion-Recognition-in-Conversations-ACL2023"><a href="#（1）Supervised-Adversarial-Contrastive-Learning-for-Emotion-Recognition-in-Conversations-ACL2023" class="headerlink" title="（1）Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations (ACL2023)"></a>（1）Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations (ACL2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对抗学习，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;在label-based contrastive learning中，通过捕获类内示例之间的相似性，并与其他类中的示例进行对比，学习一个泛化的表示。这样做是因为相似的情感往往具有相似的上下文和重叠的特征空间。这些直接压缩每个类的特征空间的技术很可能会伤害每个情感的细粒度特征，从而限制泛化能力。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p>①首先，在原始样本上执行CAT策略和SCL目标，得到“contrast-aware adversarial perturbations”。在依赖上下文的对话场景中，直接生成对抗样本会干扰语句之间的相关性，不利于上下文理解。因此，本文提出“contextual adversarial training (CAT) strategy”，用来<strong>自适应地生成与上下文有关的worst-case samples</strong>，并从上下文中提取更多样化的特征：</p><p><img src="F:/files/myBlog/images/image-20241011200245367.png" alt="image-20241011200245367"></p><p>内层max部分让模型最大化失误，找到好的对抗扰动（最能欺骗模型，寻找让模型最脆弱的输入），外层min部分最小化损失，提高模型鲁棒性。</p><p><img src="data:image/png;base64,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" alt="img"></p><p>g是样本u的梯度更新方向，g提供了扰动方向，$\epsilon$ 是扰动的大小，确保$r_{c-adv}$ 大小在epsilon之内。归一化后的梯度 $\frac{g}{|g|_q}$  是一个长度为 1 的向量，保持与原始梯度 g 相同的方向，但大小（范数）被缩放为 1。这一步的目的是确保扰动的方向与梯度方向一致，但扰动的大小是受限的（通过乘以 $\epsilon$  控制），从而符合扰动的范数约束条件。</p><p>②接着，<strong>把这个perturbation放在hidden layer上</strong>，得到“hard positive examples”。</p><p><img src="F:/files/myBlog/images/image-20241011194600472.png" alt="image-20241011194600472"></p><p>③我们在获得的对抗样本上，以及原始样本上，都使用一种软SCL来最大化具有相同标签的样本表示的一致性。在这个联合目标下，该网络可以有效地学习标签一致的特征，并实现更好的泛化：</p><p><img src="F:/files/myBlog/images/image-20241011201456919.png" alt="image-20241011201456919"></p><p><img src="F:/files/myBlog/images/image-20241011202105302.png" alt="image-20241011202105302"></p><p>④总结：本文方法SACL与其他方法比较：有对抗样本，相对CL也比较soft</p><p><img src="F:/files/myBlog/images/image-20241011172018085.png" alt="image-20241011172018085"></p><p>从下图也可以看出模型的训练过程：左边是original samples，不同类之间pull apart，相同类之间pull closer，original samples产生对抗扰动，放到双向LSTM内部，产生对抗样本，对抗样本也是一样soft-SCL学习。原始样本和对抗样本有一个对抗的学习过程。</p><p><img src="F:/files/myBlog/images/image-20241011195052638.png" alt="image-20241011195052638"></p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;ERC</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;IEMOCAP，MELD，EmoryNLP（多人对话，用LSTM来提取特征party和utterance）</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241011170951060.png" alt="image-20241011170951060"></p><p><img src="F:/files/myBlog/images/image-20241011171003626.png" alt="image-20241011171003626"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/zerohd4869/SACL">https://github.com/zerohd4869/SACL</a></p><h5 id="（2）VLATTACK-Multimodal-Adversarial-Attacks-on-Vision-Language-Tasks-via-Pre-trained-Models-NeurIPS-2023"><a href="#（2）VLATTACK-Multimodal-Adversarial-Attacks-on-Vision-Language-Tasks-via-Pre-trained-Models-NeurIPS-2023" class="headerlink" title="（2）VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models (NeurIPS 2023)"></a>（2）VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models (NeurIPS 2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对抗攻击，预训练模型</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;现有的在VL任务中进行对抗性攻击的工作主要是在白盒设置下，攻击者可以访问微调模型的梯度信息。然而，在更现实的场景中，恶意攻击者可能只能访问通过第三方发布的公共预训练模型。攻击者不会对在私有数据集上微调的下游 VL 模型学习的参数有任何先验知识。为了弥补这一显着的局限性，我们研究了一种新的但实用的攻击范式——在预训练的 VL 模型上生成对抗性扰动，以攻击在预训练模型上微调的各种黑盒下游任务。</p><p>然而，这样的攻击设置并不简单，面临以下挑战： (1)特定任务的挑战。预训练的VL模型通常用于微调不同的下游任务，这要求<strong>设计的攻击机制具有通用性并且能够攻击多个任务</strong>。 (2) 特定于模型的挑战。由于微调模型的参数未知，因此需要<strong>攻击方法自动学习不同模态的预训练模型和微调模型之间的对抗性可转移性</strong>。尽管跨图像模型的对抗性可转移性已经被广泛讨论，但<strong>在预训练模型中仍然很大程度上未被探索</strong>，特别是<strong>在不同模态上的扰动之间构建相互联系</strong>。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;提出了一种新的通用视觉语言攻击策略（称为 VLATTACK），以探索预训练和微调的 VL 模型之间的对抗可迁移性。整个 VLATTACK 方案融合了扰动两个层次的图像和文本：</p><p>①单模态层面<br>VLATTACK 在单一模态上独立生成扰动，遵循“从图像到文本”的顺序，因为前者可以在连续空间上扰动。单模态攻击可以有效地检测图像或文本的对抗性漏洞，从而避免对其他模态的冗余扰动。具体来说，为了充分利用预训练模型中存储的图像文本交互，我们提出了一种新颖的<strong>逐块相似性攻击（BSA）策略来攻击图像模态</strong>，该策略添加扰动以<strong>逐块扩大网络预训练模型中原始特征和扰动特征之间的距离</strong>，破坏了下游预测的通用图像文本表示。如果BSA在查询微调的黑盒模型后未能改变预测，VLATTACK将通过采用词级扰动技术来攻击文本模态。我们采用 <strong>BERT-Attack来攻击文本模态</strong>，因为它的突出性能已在许多研究中得到广泛验证。最后，如果所有文本扰动 {T′ i} 失败，VLATTACK 将生成扰动样本 T 的列表，并将它们与扰动图像 I′ 一起馈送到多模态攻击。</p><p>②多模态层面<br>如果上述攻击未能改变预测，我们将根据先前的输出在<strong>多模态级别交叉更新图像和文本扰动</strong>。所提出的迭代交叉搜索攻击（ICSA）策略通过考虑不同模态扰动之间的相互关系，以迭代方式更新图像文本扰动对（I′ i，T′ i）。 ICSA 使用从列表 T 中选择的文本扰动 T′ i 作为指导，通过采用逐块相似性攻击 (BSA) 迭代更新扰动图像 I′ i，直到新的对 (I′ i, T′ i) 使得下游任务变化的预测。此外，在多模态攻击层面，根据与良性扰动的语义相似度对文本扰动进行交叉搜索，逐渐加大方向修改的程度，以最大程度地保留原始语义。</p><p>③图像模块攻击：</p><p><img src="F:/files/myBlog/images/image-20241011205705613.png" alt="image-20241011205705613"></p><p><img src="F:/files/myBlog/images/image-20241011205716281.png" alt="image-20241011205716281"></p><h5 id="（3）Adversarial-alignment-and-graph-fusion-via-information-bottleneck-for-multimodal-emotion-recognition-in-conversations-Information-Fusion2024"><a href="#（3）Adversarial-alignment-and-graph-fusion-via-information-bottleneck-for-multimodal-emotion-recognition-in-conversations-Information-Fusion2024" class="headerlink" title="（3）Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations (Information Fusion2024)"></a>（3）Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations (Information Fusion2024)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对抗学习，对比学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;解决异质性问题</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012141927336.png" alt="image-20241012141927336"></p><p>①三个模态分别通过MLP映射得到初步的特征</p><p>②然后，我们构建了一个文本生成器和一个文本判别器。文本生成器“generator”的输入为音频特征ξ a和视频特征ξ v。文本判别器“discriminator”的输入是文本生成器生成的包含三种模态信息的融合特征。它们的loss学习目标（来源于博弈论的<strong>极小极大博弈</strong>）：</p><p><img src="F:/files/myBlog/images/image-20241012135934457.png" alt="image-20241012135934457"></p><ul><li>第一个是生成器的损失函数：生成器 Gt 的目标是生成高质量的文本使得判别器 Dt 无法分辨它们是真实的还是生成的。在设计上，生成器希望判别器对其生成的数据  给出尽可能接近1的判断值（即判别器认为它们是真实的）。所以生成器的损失函数是最小化 $log(1 - D_t(\cdot))$ ，代表生成器试图让判别器输出 $D_t(\cdot)$ 尽可能接近1。</li><li>第二个是判别器的损失函数：判别器 Dt 的任务是正确区分真实数据 T 和生成器生成的假数据 $G_t(\tilde{\xi_a})$  和 $G_t(\tilde{\xi_v})$ 。在设计上，判别器希望最大化其对真实数据的判断为“真”（即 $D_t(T)$  越接近1越好），同时希望最小化对生成数据的误判（即 $D_t(G_t(\tilde{\xi_a}))$  和 $D_t(G_t(\tilde{\xi_v}))$  应该尽量接近0）。</li></ul><p>同理再分别构建音频和视觉的生成器和判别器。</p><p>③IMCL: Intra-modal and inter-modal contrastive learning via IB</p><p>正样本：同一类别的同一模态的样本</p><p>负样本：同一类别的不同模态的样本</p><p>这里有一个问题就是，anchor与负样本的距离有可能非常大（相似度为0），这样正样本之间的距离对18公式的loss计算就不太影响了（有时候它会分得太过“远”，而忽略了一些细微的相似性），因为模型已经趋于最优了，因此加了一个约束项，负样本的相似度不能太小，要往β值靠近：</p><p><img src="F:/files/myBlog/images/image-20241012142322838.png" alt="image-20241012142322838"></p><p>④ICCL: Intra-class and Inter-class Contrastive Learning via IB：正负样本应该都是同一模态下，正样本是同一类别，负样本是不同类别。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;文章说对比学习需要计算最小化views的互信息，利用IB 来计算。但IB部分没怎么讲</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;Multimodal Emotion Recognition in Conversation</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MELD，IEMOCAP</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012142803496.png" alt="image-20241012142803496"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/yuntaoshou/AGF-IB">https://github.com/yuntaoshou/AGF-IB</a></p><h5 id="（4）Modality-to-Modality-Translation-An-Adversarial-Representation-Learning-and-Graph-Fusion-Network-for-Multimodal-Fusion-AAAI2020"><a href="#（4）Modality-to-Modality-Translation-An-Adversarial-Representation-Learning-and-Graph-Fusion-Network-for-Multimodal-Fusion-AAAI2020" class="headerlink" title="（4）Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion (AAAI2020)"></a>（4）Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion (AAAI2020)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;翻译网络，对抗学习</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;modality gap</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012144152727.png" alt="image-20241012144152727"></p><p>本文提出了<strong>新颖的对抗编码解码器来学习模态无关的嵌入空</strong>间，由于不同模态的分布（distribution）可能是不一样的，所以使用对抗训练的方法，根据编码器将源模态的分布转为目标模态的分布。不仅如此，还引入了重构损失以及分类损失对嵌入空间施加约束。当学习到表示之后，就是对于表示进行融合。本文使用了<strong>分层的图神经网络</strong>以多阶段的方法对于单模态，双模态和三模态的交互进行建模。</p><p>①Joint Embedding Space Learning</p><p>modality2modality translation：将源模态的分布转换为目标模态的分布以此可以得到一个模态无关的嵌入空间</p><p>模态的分布转换是这样的：通过优化参数$\theta_m$，来得到转换后的模态表征在学习嵌入空间中的分布$p_{\theta_{m}}\left(x_{m}^{e}\right)$.</p><p><img src="F:/files/myBlog/images/image-20241012144529980.png" alt="image-20241012144529980"></p><p>然而，不同模态的分布十分复杂且性质各异，极难通过简单的编码网络进行匹配。因此，我们利用对抗训练为变换后的分布添加约束。具体来说，文中用到了模态传译方面的知识，由模态传递所引发产生一个对抗的编码框架，也就是说，生成器从单一模态特征中生成一个较好的编码表示，而判别器则需要判别这个表示是否是目标模态所生成的。</p><p>定义了一个判别器D，其目的是将 $p_{\theta_{l}}\left(x_{l}^{e}\right)$ 分类为真，而将$p_{\theta_{a}}\left(x_{a}^{e}\right)$和$p_{\theta_{v}}\left(x_{v}^{e}\right)$ 分类为假，而生成器(它们是编码器Ea和Ev)试图欺骗判别器D，将$p_{\theta_{a}}\left(x_{a}^{e}\right)$和$p_{\theta_{v}}\left(x_{v}^{e}\right)$分类为真。对抗学习的loss：</p><p> <img src="F:/files/myBlog/images/image-20241012144848967.png" alt="image-20241012144848967"></p><p>公式2，第一项是生成器的loss，让判别器尽可能判断生成的v和a模态的分布为真；第二项是判别器的loss，让判别器把 l 模态判别正确，其他模态（a，v）判别错误。</p><p>如果判别器不能从所有模态中分辨出目标模态文本模态($p_{\theta_{l}}\left(x_{l}^{e}\right)D\left(\boldsymbol{x}<em>{a}^{e}\right) \approx D\left(\boldsymbol{x}</em>{v}^{e}\right) \approx D\left(\boldsymbol{x}_{l}^{e}\right)$ )，则各种模态的分布被成功地映射到一个模态不变的嵌入空间中。所以可以通过对抗学习来缩小modality gap。</p><p>“Transforming distributions”可能导致挖掘模态间互补信息所需的单模态信息丢失，所以为了获得在学习的嵌入空间中的模态独特信息，定义了decoder，加上encoder其实就是一个auto-encoder：</p><p><img src="F:/files/myBlog/images/image-20241012145048498.png" alt="image-20241012145048498"></p><p>综上，loss一共是有四部分，有两个是对抗学习的loss，一个是自编码器的loss，另外一个是MSE loss</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI，MOSEI，IEMOCAP</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;不好</p><p><img src="F:/files/myBlog/images/image-20241012144410789.png" alt="image-20241012144410789"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/TmacMai/ARGF_multimodal_fusion">https://github.com/TmacMai/ARGF_multimodal_fusion</a></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/%E5%A4%9A%E8%A7%86%E5%9B%BE/">多视图</category>
      
      
      <comments>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>调研MSA中的方法（一）：多视图</title>
      <link>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A4%9A%E8%A7%86%E5%9B%BE/</link>
      <guid>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A4%9A%E8%A7%86%E5%9B%BE/</guid>
      <pubDate>Mon, 03 Mar 2025 13:43:06 GMT</pubDate>
      
      <description>&lt;p&gt;这一章介绍四篇MSA中经典的结合多视图方法的文章~预告本系列共四章：多视图、对抗学习、元学习、对比学习。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>这一章介绍四篇MSA中经典的结合多视图方法的文章~预告本系列共四章：多视图、对抗学习、元学习、对比学习。</p><span id="more"></span><h4 id="1-multi-view"><a href="#1-multi-view" class="headerlink" title="1. multi-view"></a>1. multi-view</h4><h5 id="（1）FACTORIZED-CONTRASTIVE-LEARNING-Going-Beyond-Multi-view-Redundancy-NeurIPS-2023"><a href="#（1）FACTORIZED-CONTRASTIVE-LEARNING-Going-Beyond-Multi-view-Redundancy-NeurIPS-2023" class="headerlink" title="（1）FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy (NeurIPS 2023)"></a>（1）FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy (NeurIPS 2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;对比学习，重视unique information，</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;多视图学习有基础的假设“multi-view redundancy” ：模态之间贡献的信息都是与任务相关的。忽略了“多视图的非冗余和独特的信息可能是重要的”这种情况。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;</p><p><img src="F:/files/myBlog/images/image-20241012150724734.png" alt="image-20241012150724734"></p><p><img src="F:/files/myBlog/images/image-20241012150707918.png" alt="image-20241012150707918"></p><p>数据增强：</p><p><img src="F:/files/myBlog/images/image-20241012150748519.png" alt="image-20241012150748519"></p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;①可以使用更好的MI上下界来优化目标，具体参考一些文献；</p><p>②目前的数据增强方法需要手动选择增强方式，以大致满足<strong>定义 4</strong> 中的要求。可以用其他方式替代：①<strong>自动生成数据增强</strong>：可以扩展 <strong>InfoMin</strong> 方法，使其能够自动生成数据增强策略，以更好地满足定义 4 的标准。这意味着，不再需要手动选择增强方法，而是可以通过算法自动决定哪些增强方式最有效。②利用未来的多模态生成模型。</p><p>③可以衡量shared 或unique information哪个信息对任务来说更重要，然后可以给优化目标函数的项赋不一样的权重值。</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;包括医疗，MSA，sarcasm任务</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MIMIC，MOSEI，MOSI，UR-FUNNY</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/pliang279/FactorCL">https://github.com/pliang279/FactorCL</a>.</p><h5 id="（2）Gacs-Korner-Common-Information-Variational-Autoencoder-NeurIPS-2023"><a href="#（2）Gacs-Korner-Common-Information-Variational-Autoencoder-NeurIPS-2023" class="headerlink" title="（2）Gács-Körner Common Information Variational Autoencoder (NeurIPS 2023)"></a>（2）Gács-Körner Common Information Variational Autoencoder (NeurIPS 2023)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;重视common information</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;假设用 (A, B, C) 来构造两个变量 X 和 Y，使$$X &#x3D; f(A, C)$$，$$Y &#x3D; g(B, C)$$，并且 C 编码了 X 和 Y 之间的所有且仅有的互信息 $$I(X; Y)$$，没有办法求得从高维数据中编码的这个最大公因数C。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;GK-VAE</p><p><img src="F:/files/myBlog/images/image-20241012151353797.png" alt="image-20241012151353797"></p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;图像分类</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;3dshapes，dsprites</p><p><img src="F:/files/myBlog/images/image-20241012151540576.png" alt="image-20241012151540576"></p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/mjkleinman/common-vae">https://github.com/mjkleinman/common-vae</a></p><h5 id="（3）MISA-Modality-Invariant-and-Specific-Representations-for-Multimodal-Sentiment-Analysis-ACM-MM2020"><a href="#（3）MISA-Modality-Invariant-and-Specific-Representations-for-Multimodal-Sentiment-Analysis-ACM-MM2020" class="headerlink" title="（3）MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis (ACM MM2020)"></a>（3）MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis (ACM MM2020)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;重视common和unique information</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;异构性导致的distributional modality gaps</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;文章主要focus在如何有效地学习模态表示上，核心idea是将每个模态映射到两个不同的子 空间（factorized representations）。在第一个子空间中，学习模态间的共同信息（commonalities &#x2F;  common information）来减少模态间的差异。在第二个子空间中，学习每个模态私有的、特有的信 息。</p><p><img src="F:/files/myBlog/images/image-20241012151825928.png" alt="image-20241012151825928"></p><p>主要用了四个loss来学习模态表示：相似度通过内积计算</p><img src="F:/files/myBlog/images/image-20240511225739028.png" alt="image-20240511225739028" style="zoom:80%;" /><p>结合框架图来看，不同的loss起着各自的作用：</p><ul><li>Similarity Loss（$\mathcal{L}_{sim}$）：通过最小化相似性损失，减少每个模态的共享表示之间的差异，有助于将共同的跨模态特征对齐在一起。</li><li>Difference Loss（$\mathcal{L}_{diff}$）：通过施加软正交约束来确保模态不变和特定表示捕获输入的不同方面，同时还添加了模态特定向量之间的正交性约束。</li><li>Reconstruction Loss（$\mathcal{L}_{recon}$）：通过添加重构损失来确保潜在特征捕获其各自模态的细节，避免学习到无关紧要的表示。</li><li>Task Loss（$\mathcal{L}_{task}$）：用于评估训练时模型预测任务的准确性，对于分类任务使用标准的交叉熵损失，对于回归任务使用均方误差损失。</li></ul><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;没有量化共有信息和unique信息</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA，sarcasm</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI，MOSEI，FUNNY</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;很一般</p><p><img src="F:/files/myBlog/images/image-20241012152356165.png" alt="image-20241012152356165"></p><p><img src="F:/files/myBlog/images/image-20241012152405408.png" alt="image-20241012152405408"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;<a href="https://github.com/declare-lab/MISA">https://github.com/declare-lab/MISA</a></p><h5 id="（4）Multi-View-Interactive-Representations-for-Multimodal-Sentiment-Analysis-IEEE-TRANSACTIONS-ON-CONSUMER-ELECTRONICS-2024"><a href="#（4）Multi-View-Interactive-Representations-for-Multimodal-Sentiment-Analysis-IEEE-TRANSACTIONS-ON-CONSUMER-ELECTRONICS-2024" class="headerlink" title="（4）Multi-View Interactive Representations for Multimodal Sentiment Analysis (IEEE TRANSACTIONS ON CONSUMER ELECTRONICS 2024)"></a>（4）Multi-View Interactive Representations for Multimodal Sentiment Analysis (IEEE TRANSACTIONS ON CONSUMER ELECTRONICS 2024)</h5><p>&#x3D;&#x3D;标签：&#x3D;&#x3D;自监督标签生成</p><p>&#x3D;&#x3D;解决的问题：&#x3D;&#x3D;现有的多模态情感分析方法在捕捉不同交互状态下的多视角情感线索方面存在不足，导致多模态表示的表达能力受限。本文提出了一个新框架（MVIR），通过在多种交互状态下学习共享和私有的情感信息，增强了情感分析的表现。</p><p>&#x3D;&#x3D;主要工作：&#x3D;&#x3D;<strong>MVIR框架</strong>：一个创新的多模态情感分析框架，利用多任务学习来捕捉不同交互状态下的多视角交互表示。</p><p><strong>DVAWF算法</strong>：设计了双视角注意力（多头图注意力网络和多头自注意力机制）加权融合机制，促进跨模态的交互，增强特征融合。</p><p><strong>SSLGM模块</strong>：引入自监督标签生成模块，为不同交互状态生成伪标签，进一步优化模型的情感表示能力。</p><p>&#x3D;&#x3D;可以改进的地方：&#x3D;&#x3D;</p><p>&#x3D;&#x3D;任务：&#x3D;&#x3D;MSA</p><p>&#x3D;&#x3D;数据集：&#x3D;&#x3D;MOSI, MOSEI, SIMS</p><p>&#x3D;&#x3D;结果：&#x3D;&#x3D;一般</p><p><img src="F:/files/myBlog/images/image-20241012163328960.png" alt="image-20241012163328960"></p><p>&#x3D;&#x3D;代码：&#x3D;&#x3D;无</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/%E5%A4%9A%E8%A7%86%E5%9B%BE/">多视图</category>
      
      
      <comments>http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A4%9A%E8%A7%86%E5%9B%BE/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>论文阅读笔记：Divide, Conquer and Combine Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing</title>
      <link>http://example.com/2023/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0Divide-Conquer-and-Combine-Hierarchical-Feature-Fusion-Network-with-Local-and-Global-Perspectives-for-Multimodal-Affective-Computing/</link>
      <guid>http://example.com/2023/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0Divide-Conquer-and-Combine-Hierarchical-Feature-Fusion-Network-with-Local-and-Global-Perspectives-for-Multimodal-Affective-Computing/</guid>
      <pubDate>Sun, 08 Oct 2023 09:12:42 GMT</pubDate>
      
      <description>&lt;p&gt;方法的核心正如题目所说，设计了一个分层的融合网络，去学习局部和全局的信息，并且作者把这个流程定为了三个阶段：分治合——Divide，Conquer，Combine。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>方法的核心正如题目所说，设计了一个分层的融合网络，去学习局部和全局的信息，并且作者把这个流程定为了三个阶段：分治合——Divide，Conquer，Combine。</p><span id="more"></span><p>总体框架图如下：文本、图像、语音各自提取特征，在Divide阶段加入了一个滑动窗口，相当于把三个模态的特征进行了“切割”；再对切割后的小块局部特征进行外积操作（跟TFN模型一样的思想），外积就是在融合局部特征，是Conquer阶段；最后在Combine阶段设计了ABS-LSTM，用于提取全局特征。</p><p><img src="/images/image-20231007190803453.png" alt="image-20231007190803453"></p><p>接下来具体讲一下模型部分的某些细节，顺便讲一下论文提出的ABS-LSTM。</p><p>之所以ABS-LSTM能提取全局特征，是因为他特别注意前t个时间步的交互和信息，在当前输入的情况下，对前t个时间步给予权重得分。</p><p>第<code>L</code>步状态的cell和states计算如下：</p><p><img src="/images/image-20231007193954999.png" alt="image-20231007193954999"></p><p>接着就是LSTM的计算公式：</p><p><img src="/images/image-20231007194913954.png" alt="image-20231007194913954"></p><p>与传统的LSTM相比，h<del>l</del>和c<del>l</del>替换掉了h<del>l-1</del>和c<del>l-1</del>，传统的LSTM的对之前信息的融入通过上一步的h<del>l-1</del>和c张量，但ABS-LSTM对以前信息直接focus在前t个时间状态中，这样当前步对它们的关注更多。</p><p>同时为了防止之前的信息被稀释掉，在LSTM的hidden states计算中加入了Attention机制GIA：</p><p><img src="/images/image-20231007202145251.png" alt="image-20231007202145251"></p><p>w<del>h</del>和w<del>x</del>这两个参数蕴含了第l步隐藏向量和当前输入的重要性。所以在h<del>l</del>^a^的计算中，W<del>h2</del>w<del>h</del>作为权重，W<del>x2</del>w<del>x</del>作为bias，融入了这两个信息。</p><p>结果：</p><p><img src="/images/image-20231007204539523.png" alt="image-20231007204539523"></p><p>这还有一个小点，文章对滑动窗口的步长s和大小d进行了分析，发现当s大于d，也就是当有一些模态特征在融合阶段被遗留下来的时候，效果并没有很大的变化，这可以推测出模态特征有冗余的部分。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/Paper-Reading/">Paper Reading</category>
      
      
      <comments>http://example.com/2023/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0Divide-Conquer-and-Combine-Hierarchical-Feature-Fusion-Network-with-Local-and-Global-Perspectives-for-Multimodal-Affective-Computing/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>论文阅读笔记：UniMSE:
Towards Unified Multimodal Sentiment Analysis and Emotion Recognition
</title>
      <link>http://example.com/2023/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0UniMSE-Towards-Unified-Multimodal-Sentiment-Analysis-and-Emotion-Recognition/</link>
      <guid>http://example.com/2023/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0UniMSE-Towards-Unified-Multimodal-Sentiment-Analysis-and-Emotion-Recognition/</guid>
      <pubDate>Sat, 07 Oct 2023 02:50:07 GMT</pubDate>
      
      <description>&lt;p&gt;这篇文章有效结合了多模态情感分析和情绪识别两种任务。&lt;/p&gt;
&lt;p&gt;发表出处：EMNLP 2022&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>这篇文章有效结合了多模态情感分析和情绪识别两种任务。</p><p>发表出处：EMNLP 2022</p><span id="more"></span><p><del>第一篇给老师汇报的论文，现在再写下笔记，有点把呕了的东西再吃下去的感觉:joy:</del></p><p>笔记从下面五部分讲解：</p><h3 id="Research-Background"><a href="#Research-Background" class="headerlink" title="Research Background"></a>Research Background</h3><p><img src="/images/image-20231007163658519.png" alt="image-20231007163658519"></p><ul><li>从心理认知角度来看，情感分析MSA和情绪识别ERC的人类表达方式是一样的，所以这两个任务在直觉上可以进行关联和互补，但是现研究针对这两个任务的研究一般都是独立的，没有把他们关联起来。</li><li>情感Sentiment和情绪Emotion的定义：前者持续时间长，预测极性标签（positive，1.6）；后者持续时间短，预测情绪类别（joy）</li><li>&#x3D;&#x3D;Figure 1这张图初步展示了情感和情绪可以共享一个统一的表示空间，具体来说是通过具有相同的情感标签的样例的相似度来确定统一的一个lable&#x3D;&#x3D;</li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p><img src="/images/image-20231007164203289.png" alt="image-20231007164203289"></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>这部分的讲解可以分为五大部分：Method-Overall Architecture，Method-Task Formalization，Method-Pre-trained Modality Fusion(PMF) ，<strong>Method-Inter-modality Contrastive Learning</strong> ，Method- Grounding UL to MSA and ERC</p><ol><li><strong>Method-Overall Architecture</strong></li></ol><p><img src="/images/image-20231007164416156.png" alt="image-20231007164416156"></p><p><img src="/images/image-20231007164437307.png" alt="image-20231007164437307"></p><p>&#x3D;&#x3D;Figure 2有几个注意的点：训练集是都用到了MSA和ERC 的数据集，通过UL Lable集合起来。然后融合部分引进了对比学习，对比学习的过程就是以锚点为参照，在特征空间中将锚点和它的正样本拉得更近，将锚点和负样本推得更远。而在这个模型中具体来说就是以文本模态（小紫）为锚点，在同一个样例中三种模态让他们的表示相近（即pull close），不同的样例的三种模态push far，所以说这里的正样例有1个，负样例有n个&#x3D;&#x3D;</p><ol start="2"><li><strong>Method-Task Formalization</strong></li></ol><p><img src="/images/image-20231007164806498.png" alt="image-20231007164806498"></p><p>语音模态：用librosa 将原始声音输入处理成数值序列向量，提取梅尔谱图作为音频特征。</p><p>视频模态：从每个片段中提取固定的T帧，并使用在VGGface 4和AFEW数据集上预训练(有监督)的effecientNet来获取视频特征。</p><p><img src="/images/image-20231007164821984.png" alt="image-20231007164821984"></p><p>&#x3D;&#x3D;这里要注意他为什么能将MSA和ERC样本按照情感极性分为积极、中性和消极样本集。（我当时困惑了好久，以为ERC的数据集的标签只有情绪类别，哪来的情感极性，但其实这里用到的ERC数据集是有情感极性的，MSA数据集没有情感标签。大概是这样，具体可以看看代码和数据集。</p><ol start="3"><li><strong>Method-Pre-trained Modality Fusion(PMF)</strong></li></ol><p><img src="/images/image-20231007164854172.png" alt="image-20231007164854172"></p><p>M<del>i</del>为经过LSTM后得到的模态表征</p><ol start="4"><li><strong>Method-Inter-modality Contrastive Learning</strong></li></ol><p><img src="/images/image-20231007164908541.png" alt="image-20231007164908541"></p><ol start="5"><li><strong>Method- Grounding UL to MSA and ERC</strong></li></ol><p><img src="/images/image-20231007164948129.png" alt="image-20231007164948129"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/images/image-20231007165412671.png" alt="image-20231007165412671"></p><p>后两个数据集用ACC\WF1</p><p><img src="/images/image-20231007165435291.png" alt="image-20231007165435291"></p><p>效果嘎嘎好</p><p><img src="/images/image-20231007165449012.png" alt="image-20231007165449012"></p><p><img src="/images/image-20231007165500944.png" alt="image-20231007165500944"></p><h3 id="Conclusion-Limitations"><a href="#Conclusion-Limitations" class="headerlink" title="Conclusion&amp;Limitations"></a>Conclusion&amp;Limitations</h3><p><img src="/images/image-20231007165512816.png" alt="image-20231007165512816"></p><p><img src="/images/image-20231007165523868.png" alt="image-20231007165523868"></p><p>Limitation这里，样例相似度是通过计算文本模态的相似度得到的，没有用到其他模态，所以作者认为相似度这里还可以改进。</p><p>启发：特征的融合也许还可以改进，即这部分，主要还是用到拼接</p><p><img src="/images/image-20231007172822666.png" alt="image-20231007172822666"></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Papers/">Papers</category>
      
      
      <category domain="http://example.com/tags/MSA/">MSA</category>
      
      <category domain="http://example.com/tags/Paper-Reading/">Paper Reading</category>
      
      <category domain="http://example.com/tags/Emotion-Recognition/">Emotion Recognition</category>
      
      
      <comments>http://example.com/2023/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0UniMSE-Towards-Unified-Multimodal-Sentiment-Analysis-and-Emotion-Recognition/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>实验室环境搭建</title>
      <link>http://example.com/2023/10/07/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</link>
      <guid>http://example.com/2023/10/07/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</guid>
      <pubDate>Sat, 07 Oct 2023 02:23:21 GMT</pubDate>
      
      <description>&lt;p&gt;实验室配备了新的电脑，故此记录实验室环境的配置。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>实验室配备了新的电脑，故此记录实验室环境的配置。</p><span id="more"></span><h3 id="服务器连接"><a href="#服务器连接" class="headerlink" title="服务器连接"></a>服务器连接</h3><ol><li>创建服务器账号，需要先生成公钥（命令行生成</li><li>用mobaxterm连接服务器账号，填写好配置信息，连接成功<ul><li>这里出现一个问题，当时生成公钥配置了密码，一直permission denied（应该是设置权限问题，找Administrator</li></ul></li></ol><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>Anaconda+VSCode</p><p>选择原因：Anaconda内嵌了Python，而且可以管理环境；VSCode可以通过插件连接服务器（Pycharm要专业版收费:cry:</p><p>找了几个教程手把手操作，还在Anaconda分别新建了Pytorch和Tensorflow环境（注意cpu和gpu版本）。但是忘记保存下来了&#x3D;&#x3D;</p><h3 id="conda命令行"><a href="#conda命令行" class="headerlink" title="conda命令行"></a>conda命令行</h3><p><strong>创建虚拟环境</strong>：yida_cv是我虚拟环境的名字，你取什么名字都OK，最好能够标记好环境。<br><code>conda create -n yida_cv python=3.6 </code></p><p><strong>激活&#x2F;切换虚拟环境</strong><br><code>conda activate yida_cv</code></p><p><strong>退出并进入base环境</strong><br><code>conda deactivate </code></p><p><strong>查看已有的虚拟环境</strong><br><code>conda env list</code></p><p><strong>删除虚拟环境</strong><br><code>conda remove -n yida_cv --all</code></p><hr><p>参考更多命令行：<br><a href="https://blog.csdn.net/weixin_43312117/article/details/123431626">https://blog.csdn.net/weixin_43312117/article/details/123431626</a></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Learning/">Learning</category>
      
      
      <category domain="http://example.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">环境搭建</category>
      
      
      <comments>http://example.com/2023/10/07/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>我的第一篇博客</title>
      <link>http://example.com/2023/10/05/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</link>
      <guid>http://example.com/2023/10/05/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</guid>
      <pubDate>Wed, 04 Oct 2023 16:09:47 GMT</pubDate>
      
      <description>&lt;p&gt;主要记录Linux命令~&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>主要记录Linux命令~</p><span id="more"></span><h2 id="第一篇blog"><a href="#第一篇blog" class="headerlink" title="第一篇blog"></a>第一篇blog</h2><p>主要参考🔗：<a href="https://zhuanlan.zhihu.com/p/102592286">从零开始搭建个人博客（超详细） - 知乎 (zhihu.com)</a></p><p>感谢！</p><h3 id="使用git"><a href="#使用git" class="headerlink" title="使用git"></a>使用git</h3><p>在Blog根目录下git bash 进入</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>hexo s</td><td>server 写完博客可以先本地看看，再deploy</td></tr><tr><td>hexo clean</td><td>clean 不是每一次都需要执行</td></tr><tr><td>hexo g</td><td>generate</td></tr><tr><td>hexo d</td><td>deploy 部署</td></tr><tr><td>hexo new “My New Post”</td><td>新建Blog</td></tr></tbody></table><h3 id="使用Linux命令-vim用法"><a href="#使用Linux命令-vim用法" class="headerlink" title="使用Linux命令-vim用法"></a>使用Linux命令-vim用法</h3><p>在学习的过程中，发现有一些大佬用Linux读取文件写文件一顿操作猛如虎（respect，而我还只会打开文件资源管理器，</p><p>所以在此记录下一些关于文件的Linux命令的用法：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">##for example </span><br><span class="line">pwd       ##查看当前路径</span><br><span class="line">ls -l</span><br><span class="line">cd themes/</span><br><span class="line">cd ..     ##回到上层</span><br><span class="line">vim congfig.yml</span><br><span class="line"></span><br><span class="line">##输入内容</span><br><span class="line">i   ##insert，输入i后，即可修改文件内容</span><br><span class="line">exc    ##退回</span><br><span class="line"></span><br><span class="line">#保存，但不退出vi</span><br><span class="line">:w    ##注意：不可省略</span><br><span class="line"></span><br><span class="line">##保存并退出vi</span><br><span class="line">:wq</span><br><span class="line"></span><br><span class="line">##退出vi，但不保存更改 </span><br><span class="line">:q!</span><br><span class="line"></span><br><span class="line">##用其他文件名保存                        </span><br><span class="line">:w filename</span><br><span class="line"></span><br><span class="line">##在现有文件中保存并覆盖该文件    </span><br><span class="line">:w! filename</span><br><span class="line"></span><br><span class="line">##跳转</span><br><span class="line">shift + g  ## 到最后一行</span><br><span class="line">gg         ## 首行</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Learning/">Learning</category>
      
      
      <category domain="http://example.com/tags/Linux/">Linux</category>
      
      
      <comments>http://example.com/2023/10/05/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
