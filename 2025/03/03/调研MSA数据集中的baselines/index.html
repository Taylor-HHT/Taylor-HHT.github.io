<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="调研MSA中经典数据集的baselines，分三模态（视频）和两模态（图文）数据集。">
<meta property="og:type" content="article">
<meta property="og:title" content="调研MSA数据集中的baselines">
<meta property="og:url" content="http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84baselines/index.html">
<meta property="og:site_name" content="Welcome Taylor">
<meta property="og:description" content="调研MSA中经典数据集的baselines，分三模态（视频）和两模态（图文）数据集。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/image-20231030181210261.png">
<meta property="og:image" content="http://example.com/images/image-20231030182920474.png">
<meta property="og:image" content="http://example.com/images/image-20231030183144555.png">
<meta property="og:image" content="http://example.com/images/image-20231030183223356.png">
<meta property="og:image" content="http://example.com/images/image-20231012182908372.png">
<meta property="og:image" content="http://example.com/images/image-20231011232040761.png">
<meta property="og:image" content="http://example.com/images/image-20231012184838921.png">
<meta property="og:image" content="http://example.com/images/image-20231011230448945.png">
<meta property="og:image" content="http://example.com/images/image-20231030143908609.png">
<meta property="og:image" content="http://example.com/images/image-20231030150022824.png">
<meta property="og:image" content="http://example.com/images/image-20231030144213868.png">
<meta property="og:image" content="http://example.com/images/image-20231030151610923.png">
<meta property="og:image" content="http://example.com/images/image-20231030152115001.png">
<meta property="og:image" content="http://example.com/images/image-20231030165549886.png">
<meta property="og:image" content="http://example.com/images/image-20231014105211675.png">
<meta property="og:image" content="http://example.com/images/image-20231011220840269.png">
<meta property="og:image" content="http://example.com/images/image-20231013194821967.png">
<meta property="og:image" content="http://example.com/images/image-20231013164546125.png">
<meta property="og:image" content="http://example.com/images/image-20231013201150207.png">
<meta property="og:image" content="http://example.com/images/image-20231013201308817.png">
<meta property="og:image" content="http://example.com/images/image-20231013200021586.png">
<meta property="og:image" content="http://example.com/images/image-20231013200116329.png">
<meta property="og:image" content="http://example.com/images/image-20231030150022824.png">
<meta property="og:image" content="http://example.com/images/image-20231013195149777.png">
<meta property="og:image" content="http://example.com/images/image-20231014100347947.png">
<meta property="og:image" content="http://example.com/images/image-20231014100839586.png">
<meta property="og:image" content="http://example.com/images/image-20231012190824601.png">
<meta property="og:image" content="http://example.com/images/image-20231016110838142.png">
<meta property="og:image" content="http://example.com/images/image-20231016111433167.png">
<meta property="og:image" content="http://example.com/images/image-20231016112352137.png">
<meta property="og:image" content="http://example.com/images/image-20231016112101644.png">
<meta property="og:image" content="http://example.com/images/image-20231013194821967.png">
<meta property="og:image" content="http://example.com/images/image-20231013164546125.png">
<meta property="og:image" content="http://example.com/images/image-20231030160710263.png">
<meta property="og:image" content="http://example.com/images/image-20231012185451218.png">
<meta property="og:image" content="http://example.com/images/image-20231016163709184.png">
<meta property="og:image" content="http://example.com/images/image-20231016163057378.png">
<meta property="og:image" content="http://example.com/images/image-20231016163118305.png">
<meta property="og:image" content="http://example.com/images/image-20231013205006912.png">
<meta property="og:image" content="http://example.com/images/image-20231016155308922.png">
<meta property="og:image" content="http://example.com/images/image-20231016163605418.png">
<meta property="og:image" content="http://example.com/images/image-20231014105627893.png">
<meta property="og:image" content="http://example.com/images/image-20231013201317958.png">
<meta property="og:image" content="http://example.com/images/image-20231014110504142.png">
<meta property="og:image" content="http://example.com/images/image-20231014170333642.png">
<meta property="og:image" content="http://example.com/images/image-20231014104337227.png">
<meta property="og:image" content="http://example.com/images/image-20231013214621637.png">
<meta property="article:published_time" content="2025-03-03T14:14:00.000Z">
<meta property="article:modified_time" content="2025-04-14T06:27:07.464Z">
<meta property="article:author" content="Taylor">
<meta property="article:tag" content="MSA">
<meta property="article:tag" content="数据集">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20231030181210261.png">

<link rel="canonical" href="http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84baselines/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>调研MSA数据集中的baselines | Welcome Taylor</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="Welcome Taylor" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome Taylor</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/03/%E8%B0%83%E7%A0%94MSA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84baselines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Taylor">
      <meta itemprop="description" content="I want auroras and sad prose.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome Taylor">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          调研MSA数据集中的baselines
        </h1>

        <div class="post-meta">

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-03 22:14:00" itemprop="dateCreated datePublished" datetime="2025-03-03T22:14:00+08:00">2025-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-14 14:27:07" itemprop="dateModified" datetime="2025-04-14T14:27:07+08:00">2025-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>调研MSA中经典数据集的baselines，分三模态（视频）和两模态（图文）数据集。</p>
<span id="more"></span>
<h2 id="视频三模态数据集"><a href="#视频三模态数据集" class="headerlink" title="视频三模态数据集"></a>视频三模态数据集</h2><h3 id="MOSEI"><a href="#MOSEI" class="headerlink" title="MOSEI"></a>MOSEI</h3><ol>
<li>UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition</li>
<li>Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment</li>
<li>Multi-Modality Multi-Loss Fusion Network</li>
<li>A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis</li>
<li>Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation</li>
</ol>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>MAE</th>
<th>F1</th>
<th>Ref Type</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/unimse-towards-unified-multimodal-sentiment">UniMSE</a></td>
<td>87.50</td>
<td>0.523</td>
<td>87.46</td>
<td>2022</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speech-text-dialog-pre-training-for-spoken">SPECTRA</a></td>
<td>87.34</td>
<td></td>
<td></td>
<td>2023</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multi-modality-multi-loss-fusion-network">MMML</a></td>
<td>86.73</td>
<td>0.517</td>
<td>86.49</td>
<td>2023</td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-transformer-based-joint-encoding-for-1">Transformer-based joint-encoding</a></td>
<td>82.48</td>
<td></td>
<td></td>
<td>2020</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mmlatch-bottom-up-top-down-fusion-for"> MMLatch</a></td>
<td>82.4</td>
<td>0.7</td>
<td></td>
<td>2021</td>
<td>√</td>
</tr>
</tbody></table>
<h3 id="MOSI"><a href="#MOSI" class="headerlink" title="MOSI"></a>MOSI</h3><ol>
<li>Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment</li>
<li>UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition</li>
<li>Cross-Modal BERT for Text-Audio Sentiment Analysis</li>
<li>Gated Mechanism for Attention Based Multimodal Sentiment Analysis</li>
<li>Multimodal Transformer for Unaligned Multimodal Language Sequences</li>
</ol>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>MAE</th>
<th>F1</th>
<th>Ref Type</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speech-text-dialog-pre-training-for-spoken">SPECTRA</a></td>
<td>87.50</td>
<td></td>
<td></td>
<td>2023</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/unimse-towards-unified-multimodal-sentiment">UniMSE</a></td>
<td>86.9</td>
<td></td>
<td>86.42</td>
<td>2022</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-modal-bert-for-text-audio-sentiment">CM-BERT</a></td>
<td>84.5%</td>
<td></td>
<td>84.5%</td>
<td>2020</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gated-mechanism-for-attention-based"> Proposed: B2 + B4 w&#x2F; multimodal fusion</a></td>
<td>83.91%</td>
<td></td>
<td>81.17</td>
<td>2020</td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190600295">MulT</a></td>
<td>83%</td>
<td></td>
<td>82.8</td>
<td>2019</td>
<td>√</td>
</tr>
</tbody></table>
<h3 id="MELD"><a href="#MELD" class="headerlink" title="MELD"></a>MELD</h3><p>Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also <strong>encompasses audio and visual modality</strong> along with text. <strong>MELD has more than 1400 dialogues and 13000 utterances from Friends TV series</strong>. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been <strong>labeled by any of these seven emotions – Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral)</strong> annotation for each utterance.</p>
<p><a target="_blank" rel="noopener" href="https://affective-meld.github.io/">https://affective-meld.github.io/</a></p>
<ol>
<li><p>InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework</p>
</li>
<li><p>Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation</p>
</li>
<li><p>Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention</p>
</li>
<li><p>A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations</p>
</li>
<li><p>M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation</p>
</li>
<li><p>CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>Model</th>
<th>F1</th>
<th>Accuracy</th>
<th>Ref Type</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instructerc-reforming-emotion-recognition-in">InstructERC</a></td>
<td>69.15</td>
<td></td>
<td><em>arXiv</em>2023</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/supervised-prototypical-contrastive-learning">SPCL-CL-ERC</a></td>
<td>67.25</td>
<td></td>
<td>acl2022</td>
<td>√</td>
</tr>
<tr>
<td>DF-ERC</td>
<td>67.03</td>
<td></td>
<td><em>arXiv</em>2023</td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-dialogue-understanding-with-1">HiDialog</a></td>
<td>66.96</td>
<td></td>
<td><em>arXiv</em>2023</td>
<td>√</td>
</tr>
<tr>
<td>DualGATs</td>
<td>66.9</td>
<td></td>
<td>ACL2023</td>
<td></td>
</tr>
<tr>
<td>MultiEMO</td>
<td>66.74</td>
<td></td>
<td>ACL2023</td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-facial-expression-aware-multimodal-multi">FacialMMT</a></td>
<td>66.73</td>
<td></td>
<td>acl2023</td>
<td>√</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2fnet-multi-modal-fusion-network-for-emotion">M2FNet</a></td>
<td>66.71</td>
<td>67.85</td>
<td>CVPR2022</td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cfn-esa-a-cross-modal-fusion-network-with">CFN-ESA</a></td>
<td>66.70</td>
<td>67.85</td>
<td><em>arXiv</em>2023</td>
<td></td>
</tr>
<tr>
<td>UniMSE</td>
<td>65.51</td>
<td></td>
<td>acl2022</td>
<td></td>
</tr>
<tr>
<td>SACL-LSTM</td>
<td>58.44</td>
<td></td>
<td>ACL2023</td>
<td></td>
</tr>
</tbody></table>
<p>MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations ACL2023</p>
<p>66.74</p>
<p><img src="/images/image-20231030181210261.png" alt="image-20231030181210261"></p>
<p>DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations acl2023</p>
<p>66.9</p>
<p><img src="/images/image-20231030182920474.png" alt="image-20231030182920474"></p>
<p>Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations acl2023</p>
<p>58.44</p>
<p><img src="/images/image-20231030183144555.png" alt="image-20231030183144555"></p>
<p>Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition(<em>arXiv</em>2023)</p>
<p>67.03</p>
<p><img src="/images/image-20231030183223356.png" alt="image-20231030183223356"></p>
<h2 id="图文两模态数据集"><a href="#图文两模态数据集" class="headerlink" title="图文两模态数据集"></a>图文两模态数据集</h2><h3 id="yelp"><a href="#yelp" class="headerlink" title="yelp"></a>yelp</h3><p>源数据集论文：VistaNet: Visual aspect attention network for multimodal sentiment analysis</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Ref Type</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>UsbVisdaNet</td>
<td>62.83</td>
<td>mdpi2023</td>
<td></td>
</tr>
<tr>
<td>VisdaNet</td>
<td>62.32</td>
<td>mdpi2023</td>
<td></td>
</tr>
<tr>
<td>SFNN</td>
<td>60.8</td>
<td><em>CACRE</em>2020</td>
<td></td>
</tr>
<tr>
<td>GAFN</td>
<td>60.1</td>
<td><em>Knowledge-Based Systems</em>2022</td>
<td></td>
</tr>
<tr>
<td>VistaNet*</td>
<td>59.91</td>
<td>AAAI2019</td>
<td></td>
</tr>
<tr>
<td>HAN</td>
<td>57.33</td>
<td>NAACL2016</td>
<td></td>
</tr>
</tbody></table>
<p>UsbVisdaNet: User Behavior Visual Distillation and Attention Network for Multimodal Sentiment Classification</p>
<p><img src="/images/image-20231012182908372.png" alt="image-20231012182908372"></p>
<p>VisdaNet: Visual Distillation and Attention Network for Multimodal Sentiment Classification</p>
<p><img src="/images/image-20231011232040761.png" alt="image-20231011232040761"></p>
<p>SFNN: Semantic Features Fusion Neural Network for Multimodal Sentiment Analysis</p>
<p><img src="/images/image-20231012184838921.png" alt="image-20231012184838921"></p>
<p>Gated attention fusion network for multimodal sentiment classification</p>
<p><img src="/images/image-20231011230448945.png" alt="image-20231011230448945"></p>
<p> <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3539597.3570437">Concept-oriented transformers for visual sentiment analysis</a> ACM WSDM2023</p>
<p>70.3</p>
<p><img src="/images/image-20231030143908609.png" alt="image-20231030143908609"></p>
<p> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9718029/">Learning disentangled representation for multimodal cross-domain sentiment analysis</a> IEEE TNNLS2022</p>
<p>69.1</p>
<p><img src="/images/image-20231030150022824.png" alt="image-20231030150022824"></p>
<p> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10149413/">Exploring Semantic Relations for Social Media Sentiment Analysis</a> IEEE&#x2F;ACM TASLP2023</p>
<p>65.03</p>
<p><img src="/images/image-20231030144213868.png" alt="image-20231030144213868"></p>
<p> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9859834/">HGLNET: A Generic Hierarchical Global-Local Feature Fusion Network for Multi-Modal Classification</a> ICME2022</p>
<p>63.92</p>
<p><img src="/images/image-20231030151610923.png" alt="image-20231030151610923"></p>
<p> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9121752/">LD-MAN: Layout-driven multimodal attention network for online news sentiment recognition</a> <em>IEEE Transactions on Multimedia</em>2020</p>
<p>61.22</p>
<p><img src="/images/image-20231030152115001.png" alt="image-20231030152115001"></p>
<p> <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3583076">Hybrid Representation and Decision Fusion towards Visual-textual Sentiment</a><em>ACM Transactions on Intelligent Systems and Technology</em>2023</p>
<p>找不到&#x3D;&#x3D;</p>
<h3 id="MVSA"><a href="#MVSA" class="headerlink" title="MVSA"></a>MVSA</h3><p>&#x3D;&#x3D;MVSA-multiple&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;MVSA-single&#x3D;&#x3D;</p>
<p>ACL2023 Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection</p>
<p>源数据集论文：Sentiment analysis on multi-view social data</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>single-a</th>
<th>single-f1</th>
<th>mul-a</th>
<th>mul-f1</th>
<th>ref type</th>
<th>code</th>
</tr>
</thead>
<tbody><tr>
<td>DVW Bert</td>
<td></td>
<td></td>
<td>93.66</td>
<td>93.48</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MAMN</td>
<td>76.57</td>
<td>76.08</td>
<td>78.34</td>
<td>77.92</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ITIN</td>
<td>75.19</td>
<td>74.97</td>
<td>73.52</td>
<td>73.49</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MGNNS</td>
<td>73.77</td>
<td>72.7</td>
<td>72.49</td>
<td>69.34</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MVAN-M</td>
<td>72.98</td>
<td>72.98</td>
<td>72.36</td>
<td>72.3</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CLMLF</td>
<td>75.33</td>
<td>73.46</td>
<td>72</td>
<td>69.83</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CMCN</td>
<td>73.61</td>
<td>75.03</td>
<td>70.45</td>
<td>74.77</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Co-MN-Hop6</td>
<td>70.51</td>
<td>70.01</td>
<td>68.92</td>
<td>68.83</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MultiSentiNet</td>
<td>69.84</td>
<td>69.63</td>
<td>68.86</td>
<td>68.11</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>ACL2023 Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection</p>
<p>72.07</p>
<p><img src="/images/image-20231030165549886.png" alt="image-20231030165549886"></p>
<p>Multi-Level Attention Map Network for Multimodal Sentiment Analysis</p>
<p><img src="/images/image-20231014105211675.png" alt="image-20231014105211675"></p>
<p>Multimodal Sentiment Analysis With Image-Text Interaction Network</p>
<p><a target="_blank" rel="noopener" href="https://orca.cardiff.ac.uk/id/eprint/148383/1/ITIN-Final.pdf">https://orca.cardiff.ac.uk/id/eprint/148383/1/ITIN-Final.pdf</a></p>
<p><img src="/images/image-20231011220840269.png" alt="image-20231011220840269"></p>
<p>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks</p>
<p><img src="/images/image-20231013194821967.png" alt="image-20231013194821967"></p>
<p>Image-text Multimodal Emotion Classification via Multi-view Attentional Network</p>
<p><img src="/images/image-20231013164546125.png" alt="image-20231013164546125"></p>
<p>CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection</p>
<p><img src="/images/image-20231013201150207.png" alt="image-20231013201150207"></p>
<p>Cross-Modal Complementary Network with Hierarchical Fusion for Multimodal Sentiment Classification</p>
<p><img src="/images/image-20231013201308817.png" alt="image-20231013201308817"></p>
<p>A Co-Memory Network for Multimodal Sentiment Analysis </p>
<p><img src="/images/image-20231013200021586.png" alt="image-20231013200021586"></p>
<p>MultiSentiNet: A Deep Semantic Network for Multimodal Sentiment Analysis</p>
<p><img src="/images/image-20231013200116329.png" alt="image-20231013200116329"></p>
<p> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9718029/">Learning disentangled representation for multimodal cross-domain sentiment analysis</a> IEEE TNNLS2022</p>
<p><img src="/images/image-20231030150022824.png" alt="image-20231030150022824"></p>
<p>MVSA-mul</p>
<p>Sentiment-aware multimodal pre-training for multimodal sentiment analysis</p>
<p><img src="/images/image-20231013195149777.png" alt="image-20231013195149777"></p>
<p>MVSA-SINGLE</p>
<p>Multi-Model Fusion Framework Using Deep Learning for Visual-Textual Sentiment Classification</p>
<p><img src="/images/image-20231014100347947.png" alt="image-20231014100347947"></p>
<p>Exploring Multimodal Sentiment Analysis via CBAM Attention and Double-layer BiLSTM Architecture</p>
<p><img src="/images/image-20231014100839586.png" alt="image-20231014100839586"></p>
<p>Feature-guided Multimodal Sentiment Analysis towards Industry 4.0</p>
<p><img src="/images/image-20231012190824601.png" alt="image-20231012190824601"></p>
<h3 id="TumEmo"><a href="#TumEmo" class="headerlink" title="TumEmo"></a>TumEmo</h3><table>
<thead>
<tr>
<th>Model</th>
<th>accuracy</th>
<th>f1</th>
<th>ref type</th>
<th>code</th>
</tr>
</thead>
<tbody><tr>
<td>MLP-FEF</td>
<td>77.74</td>
<td>77.77</td>
<td><em>arXiv</em>2022</td>
<td></td>
</tr>
<tr>
<td>MULSER</td>
<td>77.58</td>
<td>77.55</td>
<td><em>IEEE Transactions on Multimedia</em>2022</td>
<td></td>
</tr>
<tr>
<td>BIT</td>
<td>71.62</td>
<td>71.68</td>
<td><em>IJCNN</em>2023</td>
<td></td>
</tr>
<tr>
<td>MCFIT</td>
<td>70.85</td>
<td>70.79</td>
<td><em>Knowledge-Based Systems</em>2023</td>
<td></td>
</tr>
<tr>
<td>MGNNS</td>
<td>66.72</td>
<td>66.69</td>
<td>ACL-IJCNLP2021</td>
<td></td>
</tr>
<tr>
<td>MVAN-M*</td>
<td>66.46</td>
<td>63.39</td>
<td><em>IEEE Transactions on Multimedia</em>2020</td>
<td></td>
</tr>
</tbody></table>
<p>Improving Visual-textual Sentiment Analysis by Fusing Expert Features （<em>arXiv</em>2022）</p>
<p><img src="/images/image-20231016110838142.png" alt="image-20231016110838142"></p>
<p>Multimodal Emotion Classification with Multi-level Semantic Reasoning Network （<em>IEEE Transactions on Multimedia</em>2022）</p>
<p><img src="/images/image-20231016111433167.png" alt="image-20231016111433167"></p>
<p>BIT: Improving Image-text Sentiment Analysis via Learning Bidirectional Image-text Interaction （<em>IJCNN</em>2023）</p>
<p><img src="/images/image-20231016112352137.png" alt="image-20231016112352137"></p>
<p>Collaborative fine-grained interaction learning for image–text sentiment analysis （<em>Knowledge-Based Systems</em>2023）</p>
<p><img src="/images/image-20231016112101644.png" alt="image-20231016112101644"></p>
<p>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks（ACL-IJCNLP2021）</p>
<p><img src="/images/image-20231013194821967.png" alt="image-20231013194821967"></p>
<p>Image-text Multimodal Emotion Classification via Multi-view Attentional Network （<em>IEEE Transactions on Multimedia</em>）</p>
<p><img src="/images/image-20231013164546125.png" alt="image-20231013164546125"></p>
<p><del>Few-Shot Multi-Modal Sentiment Analysis with Prompt-Based Vision-Aware Language Modeling</del></p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.06607">Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts</a> arxiv2022</p>
<p>58.06</p>
<p><img src="/images/image-20231030160710263.png" alt="image-20231030160710263"></p>
<p> <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3503161.3548306">Unified multi-modal pre-training for few-shot sentiment analysis with prompt-based learning</a> ACM MM2022</p>
<p>找不到&#x3D;&#x3D;</p>
<h3 id="Twitter-15-17"><a href="#Twitter-15-17" class="headerlink" title="Twitter 15&#x2F;17"></a>Twitter 15&#x2F;17</h3><p>&#x3D;&#x3D;侧重挖掘模态关联信息&#x3D;&#x3D;</p>
<p>Joint multimodal sentiment analysis based on information relevance (Information Processing and Management 2023)</p>
<p><img src="/images/image-20231012185451218.png" alt="image-20231012185451218"></p>
<p>&#x3D;&#x3D;Target-Oriented Multimodal Sentiment Classification&#x3D;&#x3D;</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>t15-a</th>
<th>t15-f1</th>
<th>t17-a</th>
<th>t17-f1</th>
<th>ref type</th>
</tr>
</thead>
<tbody><tr>
<td>EF-CapTrBert-DE</td>
<td>77.92</td>
<td>73.9</td>
<td>72.3</td>
<td>70.2</td>
<td></td>
</tr>
<tr>
<td>CoolNet</td>
<td>79.92</td>
<td>75.28</td>
<td>71.64</td>
<td>69.58</td>
<td></td>
</tr>
<tr>
<td>TomBert</td>
<td>76.18</td>
<td>71.27</td>
<td>70.5</td>
<td>68.04</td>
<td></td>
</tr>
<tr>
<td>ESTR</td>
<td>71.36</td>
<td>64.28</td>
<td>65.8</td>
<td>62</td>
<td></td>
</tr>
</tbody></table>
<blockquote>
<p>Adapting BERT for Target-Oriented Multimodal Sentiment Classification</p>
<p><img src="/images/image-20231016163709184.png" alt="image-20231016163709184"></p>
<p>Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis</p>
<p>但这篇是基于方面的</p>
<p><img src="/images/image-20231016163057378.png" alt="image-20231016163057378"></p>
<p><img src="/images/image-20231016163118305.png" alt="image-20231016163118305"></p>
<p>Cross-modal fine-grained alignment and fusion network for multimodal aspect-based &gt; sentiment analysis</p>
<p><img src="/images/image-20231013205006912.png" alt="image-20231013205006912"></p>
<p>Entity-sensitive attention and fusion network for entity-level  multimodal sentiment &gt; &gt; classification（原论文）</p>
<p><img src="/images/image-20231016155308922.png" alt="image-20231016155308922"></p>
<p>Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation</p>
<p><img src="/images/image-20231016163605418.png" alt="image-20231016163605418"></p>
</blockquote>
<h3 id="Multi-ZOL"><a href="#Multi-ZOL" class="headerlink" title="Multi-ZOL"></a>Multi-ZOL</h3><table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>F1</th>
<th>Ref Type</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>MAMN</td>
<td>75.41</td>
<td>74.79</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CMCN</td>
<td>74.28</td>
<td>71.51</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CJMA</td>
<td>70.03</td>
<td>68.36</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ModalNet</td>
<td>62.71</td>
<td>60.94</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MIMN</td>
<td>61.59</td>
<td>60.51</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<ol>
<li>Multi-Level Attention Map Network for Multimodal Sentiment Analysis</li>
</ol>
<p><img src="/images/image-20231014105627893.png" alt="image-20231014105627893"></p>
<ol start="2">
<li>Cross-Modal Complementary Network with Hierarchical Fusion for Multimodal Sentiment Classification</li>
</ol>
<p><img src="/images/image-20231013201317958.png" alt="image-20231013201317958"></p>
<ol start="3">
<li>A Conditioned Joint-Modality Attention Fusion Approach for Multimodal Aspect-Level Sentiment Analysis</li>
</ol>
<p><img src="/images/image-20231014110504142.png" alt="image-20231014110504142"></p>
<ol start="4">
<li>ModalNet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network</li>
</ol>
<p><img src="/images/image-20231014170333642.png" alt="image-20231014170333642"></p>
<ol start="5">
<li>Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis（他提出</li>
</ol>
<p><img src="/images/image-20231014104337227.png" alt="image-20231014104337227"></p>
<ol start="6">
<li>Visual Enhancement Capsule Network for Aspect-Based Multimodal Sentiment Analysis</li>
</ol>
<p><img src="/images/image-20231013214621637.png" alt="image-20231013214621637"></p>

    </div>

    
    
    

    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/MSA/" rel="tag"># MSA</a>
              <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" rel="tag"># 数据集</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/03/03/%E8%B0%83%E7%A0%94MSA%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="prev" title="调研MSA中的方法（四）：对比学习">
      <i class="fa fa-chevron-left"></i> 调研MSA中的方法（四）：对比学习
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/03/03/%E4%BB%8B%E7%BB%8DMSA%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%95%B0%E6%8D%AE%E9%9B%86/" rel="next" title="介绍MSA中经典数据集">
      介绍MSA中经典数据集 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%86%E9%A2%91%E4%B8%89%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.</span> <span class="nav-text">视频三模态数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MOSEI"><span class="nav-number">1.1.</span> <span class="nav-text">MOSEI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MOSI"><span class="nav-number">1.2.</span> <span class="nav-text">MOSI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MELD"><span class="nav-number">1.3.</span> <span class="nav-text">MELD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E6%96%87%E4%B8%A4%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.</span> <span class="nav-text">图文两模态数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#yelp"><span class="nav-number">2.1.</span> <span class="nav-text">yelp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MVSA"><span class="nav-number">2.2.</span> <span class="nav-text">MVSA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TumEmo"><span class="nav-number">2.3.</span> <span class="nav-text">TumEmo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Twitter-15-17"><span class="nav-number">2.4.</span> <span class="nav-text">Twitter 15&#x2F;17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-ZOL"><span class="nav-number">2.5.</span> <span class="nav-text">Multi-ZOL</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Taylor"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Taylor</p>
  <div class="site-description" itemprop="description">I want auroras and sad prose.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:hthuang0923@gmail.com" title="E-Mail → mailto:hthuang0923@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Taylor</span>
</div>

<div class="theme-info">
  <!-- <div class="powered-by"></div> -->
  <span class="post-count">The whole site has a total of 17.4k words</span>
  <span class="separator">|</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">Total <span id="busuanzi_value_site_pv"></span> visits</span>
    <!-- <span class="post-meta-divider">.</span> -->
    <span class="separator">|</span>
    <span id="busuanzi_container_site_uv">Total <span id="busuanzi_value_site_uv"></span> visitors</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 5000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
